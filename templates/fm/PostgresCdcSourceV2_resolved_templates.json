{
  "templates": [
    {
      "template_id": "PostgresCdcSourceV2",
      "connector_type": "SOURCE",
      "connector.class": "io.debezium.connector.v2.postgresql.PostgresConnectorV2",
      "config_defs": [
        {
          "name": "database.hostname",
          "required": true,
          "documentation": "IP address or hostname of the PostgreSQL database server."
        },
        {
          "name": "database.port",
          "required": true,
          "documentation": "Port number of the PostgreSQL database server."
        },
        {
          "name": "database.user",
          "required": true,
          "documentation": "The name of the PostgreSQL database user that has the required authorization."
        },
        {
          "name": "database.password",
          "required": true,
          "documentation": "Password of the PostgreSQL database user that has the required authorization."
        },
        {
          "name": "database.dbname",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 5,
          "display_name": "Database name",
          "documentation": "The name of the PostgreSQL database from which to stream the changes.",
          "sanitizers": [
            {
              "name": "trim"
            }
          ]
        },
        {
          "name": "database.sslmode",
          "type": "STRING",
          "required": false,
          "default_value": "prefer",
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 6,
          "display_name": "SSL mode",
          "documentation": "Whether to use an encrypted connection to the PostgreSQL server. Possible settings are: `disable`, `prefer`, and `require`. \n`disable` uses an unencrypted connection. \n`prefer` attempts to use a secure (encrypted) connection first and, failing that, an unencrypted connection. \n`require` uses a secure (encrypted) connection, and fails if one cannot be established.",
          "recommended_values": [
            "prefer",
            "disable",
            "require"
          ]
        },
        {
          "name": "topic.prefix",
          "required": true,
          "documentation": "Topic prefix that provides a namespace (logical server name) for the particular PostgreSQL database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used. The connector automatically creates Kafka topics using the naming convention: `<topic.prefix>.<schemaName>.<tableName>`."
        },
        {
          "name": "slot.name",
          "type": "STRING",
          "required": false,
          "default_value": "debezium",
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 1,
          "display_name": "Slot name",
          "documentation": "The name of the PostgreSQL logical decoding slot that was created for streaming changes from a particular plug-in for a particular database/schema. The server uses this slot to stream events to the Debezium connector that you are configuring. Slot names must conform to PostgreSQL replication slot naming rules, which state: \"Each replication slot has a name, which can contain lower-case letters, numbers, and the underscore character.\""
        },
        {
          "name": "publication.name",
          "type": "STRING",
          "required": false,
          "default_value": "dbz_publication",
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 2,
          "display_name": "Publication name",
          "documentation": "The name of the PostgreSQL publication created for streaming changes when using `pgoutput`. Based on the value of ``publication.autocreate.mode`` the publication is created at start-up if it does not already exist and it includes all tables. Debezium then applies its own include/exclude list filtering, if configured, to limit the publication to change events for the specific tables of interest. The connector user must have superuser permissions to create this publication, so it is usually preferable to create the publication before starting the connector for the first time. If the publication already exists, either for all tables or configured with a subset of tables, Debezium uses the publication as it is defined."
        },
        {
          "name": "publication.autocreate.mode",
          "type": "STRING",
          "required": false,
          "default_value": "all_tables",
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 3,
          "display_name": "Publication auto-create mode",
          "documentation": "Applies only when streaming changes by using the pgoutput plug-in. Possible settings are `all_tables`, `disabled`, and `filtered`. \n`all_tables` - If a publication exists, the connector uses it. If a publication does not exist, the connector creates a publication for all tables in the database for which the connector is capturing changes. For the connector to create a publication it must access the database through a database user account that has permission to create publications and perform replications. You can create the publication using following SQL command: `CREATE PUBLICATION <publication_name> FOR ALL TABLES;`. \n`disabled` - The connector does not attempt to create a publication. A database administrator or the user configured to perform replications must have created the publication before running the connector. If the connector cannot find the publication, the connector throws an exception and stops. \n`filtered` - If a publication exists, the connector uses it. If no publication exists, the connector creates a new publication for tables that match the current filter configuration as specified by the `table.include.list`, and `table.exclude.list` connector configuration properties. For example: `CREATE PUBLICATION <publication_name> FOR TABLE <tbl1, tbl2, tbl3>`. If the publication exists, the connector updates the publication for tables that match the current filter configuration. For example: `ALTER PUBLICATION <publication_name> SET TABLE <tbl1, tbl2, tbl3>`. For the connector to alter a publication it must access the database through a database user account that has ownership of the publication and the tables it is capturing.\nNote:\n\tIf the existing regex patterns in `table.include.list`, and `table.exclude.list` match the fully qualified name of a newly created table, the connector will miss events from this new table until the publication is manually altered to include it. To avoid missing events, it is recommended to alter the publication before adding data to newly created tables.\n\tWhen configuring multiple connectors to capture different sets of tables from the database using a filtered configuration, do not use the same publication for all connectors. If multiple connectors use the same publication, the latest connector may alter the publication based on its capture list, potentially causing an incorrect publication configuration for the older connectors.",
          "recommended_values": [
            "all_tables",
            "disabled",
            "filtered"
          ]
        },
        {
          "name": "publish.via.partition.root",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 4,
          "display_name": "Publish via partition root",
          "documentation": "This configuration is applicable only when the connector is responsible for creating the publication in the source database. It determines how change events from partitioned tables are captured and emitted.\n`true` : the connector includes the publish_via_partition_root = true parameter in the publication creation statement, instructing the source database to publish change events using the root table name. As a result, changes from all partitions are emitted under the root table in the change stream.\n`false` (the default) : the publish_via_partition_root parameter is omitted, and change events are published using the individual partition names, reflecting the exact partition where each change occurred. \nNote:\n\tThe connector applies this configuration only during the initial creation of the publication. The connector ignores the changes made to this setting after the publication has been created.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "signal.data.collection",
          "type": "STRING",
          "required": false,
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 5,
          "display_name": "Signal data collection",
          "documentation": "Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use ``schemaName.tableName`` format to specify the fully-qualified collection name. Note that the connector automatically adds the signal table to the publication only if the ``publication.autocreate.mode`` is set to ``filtered`` or ``all_tables``. You will need to add it manually if the mode is set to ``disabled``."
        },
        {
          "name": "slot.failover",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "MEDIUM",
          "group": "Database config",
          "order_in_group": 6,
          "display_name": "Create failover slot",
          "documentation": "Specifies whether the connector creates a failover slot. If set to false (the default), or if the primary server runs PostgreSQL 16 or earlier, the connector does not create a failover slot.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "snapshot.mode",
          "documentation": "Specifies the criteria for running a snapshot upon startup of the connector. Possible settings are: `initial`, `never`(deprecated), `no_data`, `initial_only`, and `when_needed`. \n`initial` - The connector performs a snapshot only when no offsets have been recorded for the logical server name. \n`never` - Deprecated, use no_data instead. \n`no_data` - The connector never performs snapshots. When a connector is configured this way, its behavior when it starts is as follows. If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN has been stored, the connector starts streaming changes from the starting position available in the replication slot. The no_data snapshot mode is useful only when you know all data of interest is still reflected in the WAL. \n`initial_only` - The connector performs an initial snapshot and then stops, without processing any subsequent changes. \n`when_needed` - The connector runs a snapshot upon startup whenever it deems it necessary. That is, when no offsets are available, or when a previously recorded offset specifies an LSN that is not available with the replication slot.",
          "recommended_values": [
            "initial",
            "never",
            "no_data",
            "initial_only",
            "when_needed"
          ]
        },
        {
          "name": "heartbeat.action.query",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Connector config",
          "order_in_group": 10,
          "display_name": "Heartbeat action query",
          "documentation": "If specified, the connector executes this query on every heartbeat against the source database. The query must be a valid SQL DML statement, typically an ``INSERT`` or ``UPDATE``, that targets a dedicated heartbeat table. \nThis configuration helps address situations where capturing changes from a low-traffic database on the same host as a high-traffic one prevents Debezium from processing WAL records and acknowledging WAL positions with the database. To address this, create a heartbeat table in the low-traffic database, and set this property to a DML statement that periodically updates the table by either inserting a new row or repeatedly updating the same row. This allows the connector to receive changes from the low-traffic database and acknowledge their LSNs, preventing unbounded WAL growth on the database host. The heartbeat query executes at regular intervals, as specified by the ``heartbeat.interval.ms`` configuration property. \nNote: To enable the connector to detect and process events from the heartbeat table, ensure the table is part of the PostgreSQL publication specified by the ``publication.name`` configuration property. In addition, if you are using a filtered publication, include the table in the connector's capture set using the schema or table include configuration properties. \nNote: To uphold the principle of least privilege, grant the connector user write permissions exclusively to essential tables, such as those used for heartbeats. In line with data minimization, ensure the connector configuration is free of PII or sensitive data, as this information is not needed for system-level functions like heartbeat queries."
        }
      ],
      "connector_configs": [
        {
          "name": "database.dbname"
        },
        {
          "name": "database.sslmode"
        },
        {
          "name": "plugin.name",
          "value": "pgoutput"
        },
        {
          "name": "publication.autocreate.mode"
        },
        {
          "name": "publication.name"
        },
        {
          "name": "publish.via.partition.root"
        },
        {
          "name": "signal.data.collection"
        },
        {
          "name": "slot.max.retries",
          "value": "20"
        },
        {
          "name": "slot.name"
        },
        {
          "name": "slot.failover"
        },
        {
          "name": "slot.retry.delay.ms",
          "value": "30000"
        },
        {
          "name": "streaming.delay.ms",
          "value": "60000"
        },
        {
          "name": "slot.seek.to.known.offset.on.start",
          "value": "true"
        },
        {
          "name": "heartbeat.action.query"
        }
      ]
    },
    {
      "template_id": "common-debezium-source-v2",
      "abstract": true,
      "config_defs": [
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        },
        {
          "name": "database.hostname",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 1,
          "display_name": "Database hostname",
          "sanitizers": [
            {
              "name": "trim"
            }
          ]
        },
        {
          "name": "database.port",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 2,
          "display_name": "Database port",
          "sanitizers": [
            {
              "name": "trim"
            }
          ]
        },
        {
          "name": "database.user",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 3,
          "display_name": "Database username"
        },
        {
          "name": "database.password",
          "type": "PASSWORD",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your database?",
          "order_in_group": 4,
          "display_name": "Database password"
        },
        {
          "name": "after.state.only",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Output messages",
          "order_in_group": 4,
          "display_name": "After-state only",
          "documentation": "Controls whether the generated Kafka record should contain only the state of the row after the event occurred.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "tombstones.on.delete",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "MEDIUM",
          "group": "Output messages",
          "order_in_group": 5,
          "display_name": "Tombstones on delete",
          "documentation": "Controls whether a tombstone event should be generated after a delete event. \n`true` - a delete operation is represented by a delete event and a subsequent tombstone event. \nfalse - only a delete event is emitted. \nAfter a source record is deleted, emitting the tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case log compaction is enabled for the topic.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "topic.prefix",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we name your topic(s)?",
          "order_in_group": 1,
          "display_name": "Topic prefix",
          "documentation": "Defines the prefix to be applied to the topic name to which the connector pushes the Kafka records.",
          "sanitizers": [
            {
              "name": "trim"
            }
          ]
        },
        {
          "name": "snapshot.mode",
          "type": "STRING",
          "required": false,
          "default_value": "initial",
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 1,
          "display_name": "Snapshot mode"
        },
        {
          "name": "table.include.list",
          "type": "LIST",
          "required": false,
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 4,
          "display_name": "Tables included",
          "documentation": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form `schemaName.tableName`. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. \nTo match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. \nIf you include this property in the configuration, do not also set the ``table.exclude.list`` property.",
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "table.exclude.list",
          "type": "LIST",
          "required": false,
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 5,
          "display_name": "Tables excluded",
          "documentation": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form `schemaName.tableName`. When this property is set, the connector captures changes from every table that you do not specify. \nTo match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. \nIf you include this property in the configuration, do not set the ``table.include.list`` property.",
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "column.exclude.list",
          "type": "LIST",
          "required": false,
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 6,
          "display_name": "Columns excluded",
          "documentation": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Fully-qualified names for columns are of the form `schemaName.tableName.columnName`. \nTo match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name.",
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "event.processing.failure.handling.mode",
          "type": "STRING",
          "required": false,
          "default_value": "fail",
          "importance": "LOW",
          "group": "Connector config",
          "order_in_group": 6,
          "display_name": "Event processing failure handling mode",
          "documentation": "Specifies how the connector should react to exceptions during processing of events. Possible settings are: `fail`, `skip`, and `warn`. \n`fail` propagates the exception, indicates the offset of the problematic event, and causes the connector to stop. \n`warn` logs the offset of the problematic event, skips that event, and continues processing. \n`skip` skips the problematic event and continues processing.",
          "recommended_values": [
            "fail",
            "skip",
            "warn"
          ]
        },
        {
          "name": "schema.name.adjustment.mode",
          "type": "STRING",
          "required": false,
          "default_value": "none",
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 7,
          "display_name": "Schema name adjustment mode",
          "documentation": "Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. \n`none` does not apply any adjustment. \n`avro` replaces the characters that cannot be used in the Avro type name with underscore. \n`avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java.",
          "recommended_values": [
            "none",
            "avro",
            "avro_unicode"
          ]
        },
        {
          "name": "field.name.adjustment.mode",
          "type": "STRING",
          "required": false,
          "default_value": "none",
          "importance": "MEDIUM",
          "group": "Connector config",
          "order_in_group": 8,
          "display_name": "Field name adjustment mode",
          "documentation": "Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. \n`none` does not apply any adjustment. \n`avro` replaces the characters that cannot be used in the Avro type name with underscore. \n`avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java.",
          "recommended_values": [
            "none",
            "avro",
            "avro_unicode"
          ]
        },
        {
          "name": "heartbeat.interval.ms",
          "type": "INT",
          "required": false,
          "default_value": "0",
          "importance": "LOW",
          "group": "Connector config",
          "order_in_group": 9,
          "display_name": "Heartbeat interval (ms)",
          "documentation": "Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages."
        },
        {
          "name": "key.converter.reference.subject.name.strategy",
          "type": "STRING",
          "importance": "HIGH",
          "group": "Schema Config",
          "order_in_group": 2,
          "display_name": "Key converter reference subject name strategy",
          "default_value": "DefaultReferenceSubjectNameStrategy",
          "documentation": "Set the subject reference name strategy for key. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.",
          "recommended_values": [
            "DefaultReferenceSubjectNameStrategy",
            "QualifiedReferenceSubjectNameStrategy"
          ],
          "dependents": [
            "output.key.format"
          ]
        },
        {
          "name": "decimal.handling.mode",
          "type": "STRING",
          "required": false,
          "default_value": "precise",
          "importance": "MEDIUM",
          "group": "How should we handle data types?",
          "order_in_group": 1,
          "display_name": "Decimal handling mode",
          "documentation": "Specifies how the connector should handle values for `DECIMAL` and `NUMERIC` columns. Possible settings are: `precise`, `double`, and `string`. \n`precise` represents values by using `java.math.BigDecimal` to represent values in binary form in change events. `double` represents values by using double values, which might result in a loss of precision but which is easier to use. `string` encodes values as formatted strings, which are easy to consume but semantic information about the real type is lost.",
          "recommended_values": [
            "double",
            "precise",
            "string"
          ]
        },
        {
          "name": "time.precision.mode",
          "type": "STRING",
          "required": false,
          "default_value": "adaptive",
          "importance": "MEDIUM",
          "group": "How should we handle data types?",
          "order_in_group": 2,
          "display_name": "Time precision mode",
          "documentation": "Time, date, and timestamps can be represented with different kinds of precisions: \n`adaptive` captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column\u2019s type. \n`adaptive_time_microseconds` captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column\u2019s type. An exception is `TIME` type fields, which are always captured as microseconds. \n`connect` always represents time and timestamp values by using Kafka Connect\u2019s built-in representations for `Time`, `Date`, and `Timestamp`, which use millisecond precision regardless of the database columns' precision.",
          "recommended_values": [
            "adaptive",
            "adaptive_time_microseconds",
            "connect"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "database.hostname"
        },
        {
          "name": "database.port"
        },
        {
          "name": "database.user"
        },
        {
          "name": "database.password"
        },
        {
          "name": "topic.prefix"
        },
        {
          "name": "table.include.list"
        },
        {
          "name": "table.exclude.list"
        },
        {
          "name": "column.exclude.list"
        },
        {
          "name": "snapshot.mode"
        },
        {
          "name": "tombstones.on.delete"
        },
        {
          "name": "poll.interval.ms",
          "value": "500"
        },
        {
          "name": "max.batch.size",
          "value": "2048"
        },
        {
          "name": "event.processing.failure.handling.mode"
        },
        {
          "name": "key.converter.reference.subject.name.strategy",
          "dynamic.mapper": {
            "name": "sr.output.key.subject.reference.naming.strategy"
          }
        },
        {
          "name": "schema.name.adjustment.mode"
        },
        {
          "name": "field.name.adjustment.mode"
        },
        {
          "name": "producer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "admin.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "admin.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "transforms",
          "switch": {
            "after.state.only": {
              "true": "unwrap",
              "false": ""
            }
          }
        },
        {
          "name": "transforms.unwrap.type",
          "switch": {
            "after.state.only": {
              "true": "io.debezium.transforms.ExtractNewRecordState",
              "false": ""
            }
          }
        },
        {
          "name": "transforms.unwrap.drop.tombstones",
          "switch": {
            "after.state.only": {
              "true": "false",
              "false": ""
            }
          }
        },
        {
          "name": "transforms.unwrap.delete.handling.mode",
          "switch": {
            "after.state.only": {
              "true": "rewrite",
              "false": ""
            }
          }
        },
        {
          "name": "heartbeat.interval.ms"
        },
        {
          "name": "decimal.handling.mode"
        },
        {
          "name": "time.precision.mode"
        },
        {
          "name": "max.queue.size.in.bytes",
          "value": "209716100"
        },
        {
          "name": "custom.metric.tags",
          "value": "connector={{.logicalClusterId}},version=v2"
        },
        {
          "name": "topic.heartbeat.prefix",
          "value": "__debezium-heartbeat-{{.logicalClusterId}}"
        },
        {
          "name": "errors.max.retries",
          "value": "3"
        },
        {
          "name": "connector.endpoint",
          "value": "${database.hostname}"
        },
        {
          "name": "connector.thread.name.pattern",
          "value": "${connector.name}-${task.id}-${debezium}-${connector.class.simple}-${topic.prefix}-${functionality}"
        }
      ]
    },
    {
      "template_id": "common",
      "global_validators": [
        {
          "name": "required",
          "priority": "HIGHEST"
        },
        {
          "name": "recommended.values",
          "priority": "HIGHER"
        }
      ],
      "abstract": true,
      "config_defs": [
        {
          "name": "connector.class",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 1,
          "display_name": "Connector class"
        },
        {
          "name": "name",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 2,
          "display_name": "Connector name",
          "documentation": "Sets a name for your connector."
        },
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        },
        {
          "name": "kafka.auth.mode",
          "type": "STRING",
          "required": false,
          "default_value": "KAFKA_API_KEY",
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 1,
          "display_name": "Kafka Cluster Authentication mode",
          "documentation": "Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.",
          "recommended_values": [
            "SERVICE_ACCOUNT",
            "KAFKA_API_KEY"
          ]
        },
        {
          "name": "kafka.api.key",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka API Key",
          "documentation": "Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY."
        }
      ],
      "connector_configs": [
        {
          "name": "tasks.max"
        },
        {
          "name": "confluent.topic.bootstrap.servers",
          "value": "Placeholder value to pass connector validations"
        },
        {
          "name": "errors.log.enable",
          "value": "true"
        },
        {
          "name": "errors.log.include.messages",
          "value": "false"
        },
        {
          "name": "errors.retry.timeout",
          "value": "300000"
        },
        {
          "name": "errors.retry.delay.max.ms",
          "value": "30000"
        },
        {
          "name": "value.converter.ignore.modern.dialects",
          "value": "true"
        }
      ]
    },
    {
      "template_id": "common-kafka-connectivity",
      "abstract": true,
      "config_defs": [],
      "connector_configs": [
        {
          "name": "consumer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "producer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "producer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "consumer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "admin.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "producer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "consumer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "admin.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "consumer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "admin.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        }
      ]
    },
    {
      "template_id": "common-source",
      "abstract": true,
      "config_defs": [
        {
          "name": "kafka.service.account.id",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka Service Account",
          "documentation": "The Service Account that will be used to generate the API keys to communicate with Kafka Cluster."
        },
        {
          "name": "kafka.api.secret",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 3,
          "display_name": "Kafka API Secret",
          "documentation": "Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.",
          "dependents": [
            "kafka.api.key"
          ]
        },
        {
          "name": "datapreview.schemas.enable",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "default_value": "false",
          "display_name": "Show schemas in data preview request output",
          "group": "Kafka Cluster credentials",
          "order_in_group": 4,
          "documentation": "This config key only applies to data preview requests and governs whether the data preview output has record schema with it.\nThe visibility condition is set such that it can never be true.\nSo this key does not show in create connector UI."
        },
        {
          "name": "errors.tolerance",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "default_value": "none",
          "display_name": "errors.tolerance",
          "documentation": "Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.",
          "recommended_values": [
            "none",
            "all"
          ]
        },
        {
          "name": "producer.override.linger.ms",
          "type": "LONG",
          "required": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "producer.override.linger.ms",
          "documentation": "The producer groups together any records that arrive in between request transmissions into a single batched request. More details can be found in the documentation: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#linger-ms."
        }
      ],
      "connector_configs": [
        {
          "name": "topic.creation.default.replication.factor",
          "value": "3"
        },
        {
          "name": "topic.creation.default.partitions",
          "value": "1"
        },
        {
          "name": "errors.tolerance",
          "value": "none"
        },
        {
          "name": "producer.override.max.request.size",
          "switch": {
            "kafka.dedicated": {
              "true": "20971610",
              "false": "8388698"
            }
          }
        },
        {
          "name": "topic.creation.default.max.message.bytes",
          "switch": {
            "kafka.dedicated": {
              "true": "20971520",
              "false": "8388608"
            }
          }
        },
        {
          "name": "datapreview.schemas.enable"
        },
        {
          "name": "errors.tolerance"
        },
        {
          "name": "producer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "admin.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "admin.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "producer.override.linger.ms"
        },
        {
          "name": "consumer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "consumer.override.sasl.mechanism",
          "value": "PLAIN"
        }
      ]
    },
    {
      "template_id": "schema-registry",
      "abstract": true,
      "config_defs": [
        {
          "name": "schema.context.name",
          "type": "STRING",
          "group": "Schema Config",
          "order_in_group": 1,
          "importance": "MEDIUM",
          "display_name": "Schema context",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "default_value": "default",
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "schema.context.name",
          "type": "STRING",
          "group": "Schema Config",
          "order_in_group": 1,
          "importance": "MEDIUM",
          "display_name": "Schema context",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "default_value": "default",
          "dependents": [
            "schema.registry.url"
          ]
        }
      ],
      "connector_configs": []
    },
    {
      "template_id": "source-connector-output-data-format",
      "abstract": true,
      "config_defs": [
        {
          "name": "output.data.format",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 1,
          "display_name": "Select output record value format",
          "alias": "data.format",
          "documentation": "Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "output.data.format",
          "type": "STRING",
          "required": true,
          "default_value": "JSON",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 1,
          "display_name": "Select output record value format",
          "alias": "data.format",
          "documentation": "Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "value.converter.schemas.enable",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "alias": "schemas.enable",
          "display_name": "value.converter.schemas.enable",
          "documentation": "Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.replace.null.with.default",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "alias": "replace.null.with.default",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.replace.null.with.default",
          "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.ignore.default.for.nullables",
          "alias": "ignore.default.for.nullables",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.ignore.default.for.nullables",
          "documentation": "When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters."
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "type": "BOOLEAN",
          "documentation": "Whether to scrub invalid names by replacing invalid characters with valid characters. Applicable for Avro and Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.scrub.invalid.names"
        },
        {
          "name": "output.data.format",
          "type": "STRING",
          "required": true,
          "default_value": "JSON",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 1,
          "display_name": "Select output record value format",
          "alias": "data.format",
          "documentation": "Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "value.converter",
          "switch": {
            "output.data.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter"
            }
          }
        },
        {
          "name": "value.converter.schemas.enable",
          "switch": {
            "output.data.format": {
              "JSON": false
            }
          }
        },
        {
          "name": "value.converter.schema.registry.url",
          "switch": {
            "output.data.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.credentials.source",
          "switch": {
            "output.data.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.user.info",
          "switch": {
            "output.data.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "switch": {
            "output.data.format": {
              "AVRO": "true",
              "PROTOBUF": "false"
            }
          }
        },
        {
          "name": "value.converter.schemas.enable"
        },
        {
          "name": "value.converter.replace.null.with.default"
        },
        {
          "name": "value.converter.ignore.default.for.nullables"
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "dynamic.mapper": {
            "name": "value.converter.scrub.invalid.names.mapper"
          }
        }
      ]
    },
    {
      "template_id": "output-key-format-debezium-source",
      "abstract": true,
      "config_defs": [
        {
          "name": "output.key.format",
          "type": "STRING",
          "required": false,
          "default_value": "JSON",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 2,
          "display_name": "Output Kafka record key format",
          "alias": "key.format",
          "documentation": "Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON",
            "STRING"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "key.converter",
          "switch": {
            "output.key.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "STRING": "org.apache.kafka.connect.storage.StringConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter"
            }
          }
        },
        {
          "name": "key.converter.schemas.enable",
          "switch": {
            "output.key.format": {
              "JSON": false
            }
          }
        },
        {
          "name": "key.converter.schema.registry.url",
          "switch": {
            "output.key.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.credentials.source",
          "switch": {
            "output.key.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.user.info",
          "switch": {
            "output.key.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        }
      ]
    },
    {
      "template_id": "eos-source",
      "abstract": true,
      "config_defs": [
        {
          "name": "exactly.once.enabled",
          "type": "BOOLEAN",
          "required": false,
          "importance": "HIGH",
          "default_value": "false",
          "group": "Exactly Once Semantics",
          "order_in_group": 1,
          "docs_hidden": true,
          "display_name": "Exactly Once Enabled",
          "documentation": "If true, exactly once support is enabled for this connector.",
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "eos.configs.visible",
                "values": "false"
              },
              "metadata": {
                "visibility": "false"
              }
            }
          ]
        },
        {
          "name": "offsets.storage.topic",
          "type": "STRING",
          "required": false,
          "importance": "MEDIUM",
          "group": "Exactly Once Semantics",
          "order_in_group": 2,
          "display_name": "Offset Storage Topic",
          "documentation": "The name of the topic to be used as the offset storage topic for storing connector offsets. Defaults to 'offset-${connector}' if not set. The provided topic should not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID."
        }
      ],
      "connector_configs": [
        {
          "name": "exactly.once.enabled"
        },
        {
          "name": "offsets.storage.topic",
          "dynamic.mapper": {
            "name": "offset.replace.mapper"
          }
        },
        {
          "name": "transactional.id.prefix",
          "dynamic.mapper": {
            "name": "transactionalIdPrefix.replace.mapper"
          }
        },
        {
          "name": "offsets.storage.partitions"
        },
        {
          "name": "transaction.boundary"
        }
      ]
    },
    {
      "template_id": "csfle-source",
      "abstract": true,
      "config_defs": [
        {
          "name": "csfle.enabled",
          "type": "BOOLEAN",
          "default_value": "false",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 1,
          "docs_hidden": true,
          "display_name": "Enable Client-Side Field Level Encryption",
          "documentation": "Determines whether the connector honours CSFLE rules or not",
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "csfle.configs.visible",
                "values": "false"
              },
              "metadata": {
                "visibility": "false"
              }
            }
          ]
        },
        {
          "name": "sr.service.account.id",
          "type": "STRING",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 2,
          "docs_hidden": true,
          "display_name": "Schema Registry Service Account",
          "documentation": "Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry."
        }
      ],
      "connector_configs": [
        {
          "name": "csfle.enabled"
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.enabled": {
              "true": "ERROR"
            }
          }
        },
        {
          "name": "value.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.enabled": {
              "true": "ERROR"
            }
          }
        },
        {
          "name": "key.converter.auto.register.schemas",
          "switch": {
            "csfle.enabled": {
              "true": "false"
            }
          }
        },
        {
          "name": "key.converter.use.latest.version",
          "switch": {
            "csfle.enabled": {
              "true": "true"
            }
          }
        },
        {
          "name": "key.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        },
        {
          "name": "value.converter.auto.register.schemas",
          "switch": {
            "csfle.enabled": {
              "true": "false"
            }
          }
        },
        {
          "name": "value.converter.use.latest.version",
          "switch": {
            "csfle.enabled": {
              "true": "true"
            }
          }
        }
      ]
    },
    {
      "template_id": "super",
      "abstract": true,
      "config_defs": [
        {
          "name": "auto.restart.on.user.error",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "MEDIUM",
          "group": "Auto-restart policy",
          "order_in_group": 1,
          "display_name": "Enable Connector Auto-restart",
          "documentation": "Enable connector to automatically restart on user-actionable errors."
        },
        {
          "name": "value.converter.enhanced.avro.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information and Enums. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data",
          "type": "BOOLEAN",
          "documentation": "Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to generate an index suffix for unions. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums",
          "type": "BOOLEAN",
          "documentation": "Whether to represent enums as integers. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should be specified with an optional label. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls",
          "type": "BOOLEAN",
          "documentation": "Whether to generate a struct variable for null values. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should use primitive wrapper messages. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives",
          "type": "BOOLEAN",
          "documentation": "Whether a wrapper message should be interpreted as a raw primitive at root level. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties",
          "type": "BOOLEAN",
          "documentation": "Whether to allow additional properties for object schemas. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired",
          "type": "BOOLEAN",
          "documentation": "Whether to set non-required properties to be optional. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format",
          "type": "STRING",
          "recommended_values": [
            "BASE64",
            "NUMERIC"
          ],
          "documentation": "Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:\nBASE64 to serialize DECIMAL logical types as base64 encoded binary data and\nNUMERIC to serialize Connect DECIMAL logical type values in JSON/JSON_SR as a number representing the decimal value.",
          "group": "Additional Configs",
          "alias": "json.output.decimal.format",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.decimal.format",
          "default_value": "BASE64"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "type": "BOOLEAN",
          "documentation": "Specify if the Serializer should attempt to register the Schema.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.auto.register.schemas"
        },
        {
          "name": "value.converter.use.latest.version",
          "type": "BOOLEAN",
          "documentation": "Use latest version of schema in subject for serialization when auto.register.schemas is false.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.latest.version"
        },
        {
          "name": "value.converter.latest.compatibility.strict",
          "type": "BOOLEAN",
          "documentation": "Verify latest subject version is backward compatible when `use.latest.version` is `true`.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "type": "STRING",
          "default_value": "TopicNameStrategy",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "alias": "key.subject.name.strategy",
          "documentation": "How to construct the subject name for key schema registration.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "key.converter.key.subject.name.strategy"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "default_value": "TopicNameStrategy",
          "alias": "subject.name.strategy,value.subject.name.strategy",
          "documentation": "Determines how to construct the subject name under which the value schema is registered with Schema Registry.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.value.subject.name.strategy"
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "DefaultReferenceSubjectNameStrategy",
            "QualifiedReferenceSubjectNameStrategy"
          ],
          "default_value": "DefaultReferenceSubjectNameStrategy",
          "documentation": "Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.reference.subject.name.strategy"
        },
        {
          "name": "value.converter.allow.optional.map.keys",
          "type": "BOOLEAN",
          "documentation": "Allow optional string map key when converting from Connect Schema to Avro Schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions",
          "type": "BOOLEAN",
          "default_value": "false",
          "documentation": "Whether to flatten singleton unions. Applicable for Avro and JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2",
          "type": "BOOLEAN",
          "documentation": "Whether proto2 optionals are supported. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to flatten unions (oneofs). Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "header.converter",
          "documentation": "The converter class for the headers. This is used to serialize and deserialize the headers of the messages.",
          "recommended_values": [
            "org.apache.kafka.connect.storage.SimpleHeaderConverter",
            "org.apache.kafka.connect.storage.StringConverter",
            "org.apache.kafka.connect.json.JsonConverter",
            "org.apache.kafka.connect.converters.BooleanConverter",
            "org.apache.kafka.connect.converters.DoubleConverter",
            "org.apache.kafka.connect.converters.FloatConverter",
            "org.apache.kafka.connect.converters.IntegerConverter",
            "org.apache.kafka.connect.converters.LongConverter",
            "org.apache.kafka.connect.converters.ShortConverter"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "auto.restart.on.user.error"
        },
        {
          "name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "dynamic.mapper": {
            "name": "value.converter.auto.register.schemas.mapper"
          }
        },
        {
          "name": "value.converter.use.latest.version",
          "dynamic.mapper": {
            "name": "value.converter.use.latest.version.mapper"
          }
        },
        {
          "name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.reference.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter"
        }
      ]
    },
    {
      "template_id": "super-source",
      "abstract": true,
      "config_defs": [
        {
          "name": "producer.override.compression.type",
          "type": "STRING",
          "recommended_values": [
            "none",
            "gzip",
            "snappy",
            "lz4",
            "zstd"
          ],
          "documentation": "The compression type for all data generated by the producer. Valid values are none, gzip, snappy, lz4, and zstd.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "producer.override.compression.type"
        }
      ],
      "connector_configs": [
        {
          "name": "producer.override.compression.type"
        }
      ]
    }
  ]
}