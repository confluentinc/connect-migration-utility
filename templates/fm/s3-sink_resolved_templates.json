{
  "templates": [
    {
      "template_id": "s3-sink",
      "connector_type": "SINK",
      "connector.class": "io.confluent.connect.s3.S3SinkConnector",
      "connector_documentation_url": "https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html",
      "config_defs": [
        {
          "name": "input.key.format",
          "default_value": "BYTES"
        },
        {
          "name": "output.data.format",
          "type": "STRING",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 1,
          "default_value": "${input.data.format}",
          "default_value_provider": {
            "name": "data.format.provider"
          },
          "display_name": "Output message format",
          "documentation": "Set the output message format for values. Valid entries are AVRO, JSON, PARQUET or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO. Note that the output message format defaults to the value in the Input Message Format field. If either PROTOBUF or JSON_SR is selected as the input message format, you should select one explicitly.",
          "recommended_values": [
            "AVRO",
            "JSON",
            "BYTES",
            "PARQUET"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "aws.access.key.id",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "AWS credentials",
          "order_in_group": 2,
          "display_name": "Amazon Access Key ID"
        },
        {
          "name": "aws.secret.access.key",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "AWS credentials",
          "order_in_group": 3,
          "display_name": "Amazon Secret Access Key",
          "dependents": [
            "aws.access.key.id"
          ]
        },
        {
          "name": "s3.region",
          "type": "STRING",
          "default_value": "${kafka.region}",
          "importance": "LOW",
          "group": "Amazon S3 details",
          "order_in_group": 3,
          "display_name": "AWS S3 Region",
          "documentation": "The AWS region where the S3 bucket is defined.",
          "recommender": {
            "name": "aws.regions"
          },
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.disable",
              "arguments": {
                "config": "connector.regional.connectivity.enabled",
                "values": "true"
              },
              "metadata": {
                "isDisabled": "false"
              }
            },
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "connector.regional.connectivity.enabled",
                "values": "true"
              },
              "metadata": {
                "visibility": "true"
              }
            }
          ]
        },
        {
          "name": "s3.bucket.name",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "Amazon S3 details",
          "order_in_group": 4,
          "display_name": "Bucket name",
          "documentation": "An Amazon S3 bucket must be in the same region as your Confluent Cloud cluster."
        },
        {
          "name": "regional.connectivity",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "queryable_internal": true,
          "default_value": null,
          "custom_config_provider": {
            "name": "common.connector_region.connectivity.provider",
            "arguments": {
              "region.config": "s3.region"
            }
          }
        },
        {
          "name": "store.url",
          "type": "STRING",
          "required": false,
          "default_value": "",
          "importance": "MEDIUM",
          "group": "Amazon S3 details",
          "order_in_group": 5,
          "display_name": "Store URL",
          "documentation": "The object storage connection URL, if applicable. For example: 'https://bucket.s3-aws-region.amazonaws.com'"
        },
        {
          "name": "topics.dir",
          "group": "Organize my data by...",
          "documentation": "Configures the directory to store the data ingested from Kafka. For a file like ``s3://<s3-bucket-name>/json_logs/daily/<Topic-Name>/dt=2020-02-06/hr=09/<files>``, set ``topics.dir=json_logs/daily``, ``path.format='dt'=YYYY-MM-dd/'hr'=HH``, and ``time.interval=HOURLY``. For another file like ``s3://<s3-bucket-name>/<Topic-Name>/dt=2020-02-06/hr=09/<files>``, set ``topics.dir=\" \"``, but keep ``path.format`` and ``time.interval`` the same as in the previous example. This configures the ``topics.dir`` to a space. In the UI, enter a blank space, and use ``\" \"`` for CLI and API configurations."
        },
        {
          "name": "partitioner.class",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "default_value": "TimeBasedPartitioner",
          "group": "Organize my data by...",
          "order_in_group": 1,
          "display_name": "Partitioner Class",
          "documentation": "The partitioner to use when writing data to the Object store",
          "recommended_values": [
            "TimeBasedPartitioner",
            "FieldPartitioner"
          ]
        },
        {
          "name": "locale",
          "group": "Organize my data by...",
          "order_in_group": 2
        },
        {
          "name": "timezone",
          "group": "Organize my data by...",
          "order_in_group": 3
        },
        {
          "name": "path.format",
          "group": "Organize my data by...",
          "documentation": "This configuration is used to set the format of the data directories when partitioning with TimeBasedPartitioner. The format set in this configuration converts the Unix timestamp to a valid directory string. To organize files like this example, s3://<s3-bucket-name>/json_logs/daily/<Topic-Name>/dt=2020-02-06/hr=09/<files>, use the properties: topics.dir=json_logs/daily, path.format='dt'=YYYY-MM-dd/'hr'=HH, and time.interval=HOURLY.",
          "order_in_group": 5
        },
        {
          "name": "partition.field.name",
          "type": "LIST",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 6,
          "display_name": "Partition Field Name",
          "documentation": "The partition field name to use when partitioning with FieldPartitioner",
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "timestamp.field",
          "group": "Organize my data by...",
          "order_in_group": 8
        },
        {
          "name": "time.interval",
          "group": "Organize my data by...",
          "order_in_group": 9
        },
        {
          "name": "subject.name.strategy",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 8,
          "default_value": "TopicNameStrategy",
          "display_name": "Subject Name Strategy",
          "documentation": "Strategy used for deriving subject name from topic and record schema name.",
          "recommended_values": [
            "TopicNameStrategy",
            "TopicRecordNameStrategy"
          ]
        },
        {
          "name": "s3.schema.partition.affix.type",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 9,
          "default_value": "NONE",
          "display_name": "Schema Partition Affix Type",
          "documentation": "Append the record schema name to prefix or suffix in the s3 path after the topic name. None will not append the schema name in the s3 path.",
          "recommended_values": [
            "PREFIX",
            "SUFFIX",
            "NONE"
          ]
        },
        {
          "name": "compression.codec",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 8,
          "display_name": "Compression Type",
          "documentation": "Compression type for files written to S3.",
          "recommended_values": [
            "AVRO - deflate",
            "AVRO - snappy",
            "AVRO - bzip2",
            "PARQUET - none",
            "PARQUET - gzip",
            "PARQUET - snappy",
            "BYTES - gzip",
            "JSON - gzip"
          ]
        },
        {
          "name": "s3.compression.level",
          "type": "INT",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 9,
          "display_name": "Gzip Compression Level",
          "documentation": "Gzip compression level for files written to S3. Applied when using JSON or BYTES input.",
          "recommender": {
            "name": "compression.level"
          }
        },
        {
          "name": "behavior.on.null.values",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 8,
          "display_name": "How to handle records with null values",
          "documentation": "How to handle records with null values, e.g Kafka tombstone records. Valid options are \u2018ignore\u2019, \u2018fail\u2019 and \u2018write\u2019. Default is \u2018ignore\u2019",
          "default_value": "ignore",
          "recommended_values": [
            "ignore",
            "fail",
            "write"
          ]
        },
        {
          "name": "tombstone.encoded.partition",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 8,
          "display_name": "Tombstone Encoded Partition",
          "documentation": "Output s3 folder to write the tombstone records to. The configured partitioner would map tombstone records to this output folder.",
          "default_value": "tombstone"
        },
        {
          "name": "s3.acl.canned",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 10,
          "display_name": "An S3 canned ACL header value",
          "documentation": "An S3 canned ACL header value to apply when writing objects.",
          "recommended_values": [
            "private",
            "public-read",
            "public-read-write",
            "authenticated-read",
            "log-delivery-write",
            "bucket-owner-read",
            "bucket-owner-full-control",
            "aws-exec-read"
          ]
        },
        {
          "name": "store.kafka.keys",
          "type": "BOOLEAN",
          "required": false,
          "importance": "LOW",
          "default_value": false,
          "group": "Organize my data by...",
          "order_in_group": 11,
          "display_name": "Enable or disable writing record keys to storage",
          "documentation": "Enable or disable writing record keys to storage",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "output.keys.format",
          "type": "STRING",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 12,
          "default_value": "AVRO",
          "display_name": "Output keys format",
          "documentation": "Set the output format for keys. Valid entries are AVRO, JSON, PARQUET or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO.",
          "recommended_values": [
            "AVRO",
            "JSON",
            "BYTES",
            "PARQUET"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "store.kafka.headers",
          "type": "BOOLEAN",
          "required": false,
          "importance": "LOW",
          "default_value": false,
          "group": "Organize my data by...",
          "order_in_group": 13,
          "display_name": "Enable or disable writing record headers to storage",
          "documentation": "Enable or disable writing record headers to storage.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "output.headers.format",
          "type": "STRING",
          "importance": "HIGH",
          "group": "Output messages",
          "order_in_group": 14,
          "default_value": "AVRO",
          "display_name": "Output headers format",
          "documentation": "Set the output format for headers. Valid entries are AVRO, JSON, PARQUET or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO.",
          "recommended_values": [
            "AVRO",
            "JSON",
            "BYTES",
            "PARQUET"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "json.decimal.format",
          "type": "STRING",
          "importance": "LOW",
          "group": "Output messages",
          "order_in_group": 15,
          "default_value": "BASE64",
          "display_name": "JSON decimal format",
          "documentation": "Controls which format json converter will serialize decimals in. This value can be either 'BASE64' (default) or 'NUMERIC' and is applicable only when the output format is JSON.",
          "recommended_values": [
            "BASE64",
            "NUMERIC"
          ]
        },
        {
          "name": "s3.object.tagging",
          "type": "BOOLEAN",
          "required": false,
          "importance": "LOW",
          "default_value": false,
          "group": "Organize my data by...",
          "order_in_group": 16,
          "display_name": "Tag S3 objects offsets and record count.",
          "documentation": "Tag S3 objects with start and end offsets, as well as record count.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "s3.ssea.name",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "default_value": "",
          "group": "Amazon S3 details",
          "order_in_group": 4,
          "display_name": "The S3 Server Side Encryption Algorithm",
          "documentation": "The S3 Server Side Encryption Algorithm.",
          "recommended_values": [
            "AES256",
            "aws:kms"
          ]
        },
        {
          "name": "s3.sse.customer.key",
          "type": "PASSWORD",
          "required": false,
          "importance": "LOW",
          "default_value": null,
          "group": "Amazon S3 details",
          "order_in_group": 5,
          "display_name": "S3 Server Side (SSE-C) Key",
          "documentation": "The S3 Server Side Encryption Customer-Provided Key (SSE-C)"
        },
        {
          "name": "s3.sse.kms.key.id",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "default_value": null,
          "group": "Amazon S3 details",
          "order_in_group": 6,
          "display_name": "S3 Server Side Encryption Key",
          "documentation": "The name of the AWS Key Management Service (AWS-KMS) key to be used for server side encryption of the S3 objects. No encryption is used when no key is provided, but it is enabled when ``KMS`` is specified as encryption algorithm with a valid key name."
        },
        {
          "name": "s3.part.size",
          "type": "INT",
          "required": false,
          "importance": "HIGH",
          "default_value": 5242880,
          "group": "Amazon S3 details",
          "order_in_group": 7,
          "display_name": "Part Size in Multi-part Uploads",
          "documentation": "The Part Size(bytes) in S3 Multi-part Uploads."
        },
        {
          "name": "s3.wan.mode",
          "type": "BOOLEAN",
          "required": false,
          "importance": "MEDIUM",
          "default_value": false,
          "group": "Amazon S3 details",
          "order_in_group": 8,
          "display_name": "Use S3 accelerated endpoint",
          "documentation": "Use S3 accelerated endpoint."
        },
        {
          "name": "s3.path.style.access.enabled",
          "type": "BOOLEAN",
          "required": false,
          "importance": "MEDIUM",
          "default_value": true,
          "group": "Amazon S3 details",
          "order_in_group": 9,
          "display_name": "Enable Path Style Access to S3",
          "documentation": "Specifies whether or not to enable path style access to the bucket used by the connector"
        },
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        }
      ],
      "connector_configs": [
        {
          "name": "consumer.bootstrap.servers",
          "value": "${kafka.endpoint}"
        },
        {
          "name": "producer.bootstrap.servers",
          "value": "${kafka.endpoint}"
        },
        {
          "name": "consumer.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "consumer.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "producer.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "consumer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "consumer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "producer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "admin.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "admin.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "format.class",
          "switch": {
            "output.data.format": {
              "AVRO": "io.confluent.connect.s3.format.avro.AvroFormat",
              "JSON": "io.confluent.connect.s3.format.json.JsonFormat",
              "PARQUET": "io.confluent.connect.s3.format.parquet.ParquetFormat",
              "BYTES": "io.confluent.connect.s3.format.bytearray.ByteArrayFormat"
            }
          }
        },
        {
          "name": "keys.format.class",
          "switch": {
            "output.keys.format": {
              "AVRO": "io.confluent.connect.s3.format.avro.AvroFormat",
              "JSON": "io.confluent.connect.s3.format.json.JsonFormat",
              "PARQUET": "io.confluent.connect.s3.format.parquet.ParquetFormat",
              "BYTES": "io.confluent.connect.s3.format.bytearray.ByteArrayFormat"
            }
          }
        },
        {
          "name": "headers.format.class",
          "switch": {
            "output.headers.format": {
              "AVRO": "io.confluent.connect.s3.format.avro.AvroFormat",
              "JSON": "io.confluent.connect.s3.format.json.JsonFormat",
              "PARQUET": "io.confluent.connect.s3.format.parquet.ParquetFormat",
              "BYTES": "io.confluent.connect.s3.format.bytearray.ByteArrayFormat"
            }
          }
        },
        {
          "name": "topics"
        },
        {
          "name": "s3.schema.partition.affix.type"
        },
        {
          "name": "tasks.max"
        },
        {
          "name": "s3.region",
          "dynamic.mapper": {
            "name": "s3.region"
          }
        },
        {
          "name": "s3.bucket.name"
        },
        {
          "name": "schema.cache.size",
          "value": "1000"
        },
        {
          "name": "enhanced.avro.schema.support"
        },
        {
          "name": "s3.credentials.provider.class",
          "switch": {
            "authentication.method": {
              "Access Keys": "io.confluent.cloud.connect.storage.DummyCloudPropertiesCredentialsProvider",
              "IAM Roles": "io.confluent.provider.integration.aws.v1.ChainedAssumeRoleCredentialsProvider"
            }
          }
        },
        {
          "name": "s3.credentials.provider.customer.aws.iam.role.arn",
          "value": "${customer.aws.iam.role.arn}"
        },
        {
          "name": "s3.credentials.provider.external.id",
          "value": "${external.id}"
        },
        {
          "name": "s3.credentials.provider.confluent.aws.iam.role.arn",
          "value": "${confluent.aws.iam.role.arn}"
        },
        {
          "name": "s3.credentials.provider.middleware.external.id",
          "value": "${middleware.external.id}"
        },
        {
          "name": "s3.credentials.provider.aws.iam.assume.role.session.name",
          "value": "${aws.iam.assume.role.session.name}"
        },
        {
          "name": "s3.credentials.provider.provider.integration.max.retries",
          "value": "${provider.integration.max.retries}"
        },
        {
          "name": "s3.credentials.provider.file.path",
          "value": "/mnt/secrets/connect-external-secrets-{{.logicalClusterId}}.properties"
        },
        {
          "name": "s3.part.retries",
          "value": "10000"
        },
        {
          "name": "storage.class",
          "value": "io.confluent.connect.s3.storage.S3Storage"
        },
        {
          "name": "behavior.on.null.values"
        },
        {
          "name": "json.decimal.format"
        },
        {
          "name": "tombstone.encoded.partition"
        },
        {
          "name": "transforms",
          "value": "requireTimestampTransform"
        },
        {
          "name": "connect.meta.data",
          "value": "false"
        },
        {
          "name": "transforms.requireTimestampTransform.type",
          "value": "io.confluent.cctransforms.RequireTimestampTransform"
        },
        {
          "name": "avro.codec",
          "switch": {
            "compression.codec": {
              "AVRO - deflate": "deflate",
              "AVRO - snappy": "snappy",
              "AVRO - bzip2": "bzip2"
            }
          }
        },
        {
          "name": "parquet.codec",
          "switch": {
            "compression.codec": {
              "PARQUET - none": "none",
              "PARQUET - gzip": "gzip",
              "PARQUET - snappy": "snappy"
            }
          }
        },
        {
          "name": "s3.compression.type",
          "switch": {
            "compression.codec": {
              "BYTES - gzip": "gzip",
              "JSON - gzip": "gzip"
            }
          }
        },
        {
          "name": "s3.compression.level"
        },
        {
          "name": "schema.compatibility"
        },
        {
          "name": "s3.acl.canned"
        },
        {
          "name": "store.kafka.keys"
        },
        {
          "name": "store.kafka.headers"
        },
        {
          "name": "s3.object.tagging"
        },
        {
          "name": "s3.ssea.name"
        },
        {
          "name": "s3.sse.customer.key"
        },
        {
          "name": "s3.sse.kms.key.id"
        },
        {
          "name": "s3.part.size"
        },
        {
          "name": "s3.wan.mode"
        },
        {
          "name": "s3.path.style.access.enabled"
        },
        {
          "name": "store.url"
        },
        {
          "name": "connector.endpoint",
          "value": "${store.url}"
        },
        {
          "name": "kafka.max.partition.validation.disable"
        },
        {
          "name": "aws.access.key.id"
        },
        {
          "name": "aws.secret.access.key"
        },
        {
          "name": "connector.target.region",
          "value": "${regional.connectivity}"
        },
        {
          "name": "s3.elastic.buffer.init.capacity"
        },
        {
          "name": "s3.elastic.buffer.enable",
          "value": "true"
        },
        {
          "name": "max.write.duration.ms",
          "value": "300000"
        },
        {
          "name": "partitioner.class",
          "switch": {
            "partitioner.class": {
              "TimeBasedPartitioner": "io.confluent.connect.storage.partitioner.TimeBasedPartitioner",
              "FieldPartitioner": "io.confluent.connect.storage.partitioner.FieldPartitioner"
            }
          }
        },
        {
          "name": "partition.field.name"
        },
        {
          "name": "partitioner.max.open.files",
          "switch": {
            "partitioner.class": {
              "DEFAULT": -1,
              "FieldPartitioner": 50
            }
          }
        },
        {
          "name": "kafka.max.partitions.limit.override"
        }
      ]
    },
    {
      "template_id": "common",
      "global_validators": [
        {
          "name": "required",
          "priority": "HIGHEST"
        },
        {
          "name": "recommended.values",
          "priority": "HIGHER"
        }
      ],
      "abstract": true,
      "config_defs": [
        {
          "name": "connector.class",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 1,
          "display_name": "Connector class"
        },
        {
          "name": "name",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 2,
          "display_name": "Connector name",
          "documentation": "Sets a name for your connector."
        },
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        },
        {
          "name": "kafka.auth.mode",
          "type": "STRING",
          "required": false,
          "default_value": "KAFKA_API_KEY",
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 1,
          "display_name": "Kafka Cluster Authentication mode",
          "documentation": "Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.",
          "recommended_values": [
            "SERVICE_ACCOUNT",
            "KAFKA_API_KEY"
          ]
        },
        {
          "name": "kafka.api.key",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka API Key",
          "documentation": "Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY."
        }
      ],
      "connector_configs": [
        {
          "name": "tasks.max"
        },
        {
          "name": "confluent.topic.bootstrap.servers",
          "value": "Placeholder value to pass connector validations"
        },
        {
          "name": "errors.log.enable",
          "value": "true"
        },
        {
          "name": "errors.log.include.messages",
          "value": "false"
        },
        {
          "name": "errors.retry.timeout",
          "value": "300000"
        },
        {
          "name": "errors.retry.delay.max.ms",
          "value": "30000"
        },
        {
          "name": "value.converter.ignore.modern.dialects",
          "value": "true"
        }
      ]
    },
    {
      "template_id": "common-kafka-connectivity",
      "abstract": true,
      "config_defs": [],
      "connector_configs": [
        {
          "name": "consumer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "producer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "producer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "consumer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "admin.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "producer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "consumer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "admin.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "consumer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "admin.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        }
      ]
    },
    {
      "template_id": "common-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "kafka.service.account.id",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka Service Account",
          "documentation": "The Service Account that will be used to generate the API keys to communicate with Kafka Cluster."
        },
        {
          "name": "kafka.api.secret",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 3,
          "display_name": "Kafka API Secret",
          "documentation": "Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.",
          "dependents": [
            "kafka.api.key"
          ]
        },
        {
          "name": "topics",
          "type": "LIST",
          "required": true,
          "importance": "HIGH",
          "group": "Which topics do you want to get data from?",
          "order_in_group": 1,
          "display_name": "Topic names",
          "documentation": "Identifies the topic name or a comma-separated list of topic names.",
          "dependents": [
            "kafka.api.secret"
          ],
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "max.poll.interval.ms",
          "type": "LONG",
          "required": false,
          "importance": "LOW",
          "group": "Consumer configuration",
          "order_in_group": 1,
          "display_name": "Max poll interval(ms)",
          "default_value": "300000",
          "documentation": "The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes)."
        },
        {
          "name": "max.poll.records",
          "type": "LONG",
          "required": false,
          "importance": "LOW",
          "group": "Consumer configuration",
          "order_in_group": 2,
          "display_name": "Max poll records",
          "default_value": "500",
          "documentation": "The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records."
        },
        {
          "name": "errors.tolerance",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "default_value": "all",
          "display_name": "errors.tolerance",
          "documentation": "Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.",
          "recommended_values": [
            "none",
            "all"
          ]
        },
        {
          "name": "errors.deadletterqueue.topic.name",
          "type": "STRING",
          "importance": "LOW",
          "group": "Which topics do you want to get data from?",
          "order_in_group": 2,
          "display_name": "Dead Letter Queue Topic Name",
          "documentation": "The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.",
          "default_value": "dlq-${connector}"
        }
      ],
      "connector_configs": [
        {
          "name": "topics"
        },
        {
          "name": "errors.tolerance",
          "value": "all"
        },
        {
          "name": "errors.deadletterqueue.topic.name",
          "value": "dlq-{{.logicalClusterId}}"
        },
        {
          "name": "errors.deadletterqueue.topic.replication.factor",
          "value": "3"
        },
        {
          "name": "errors.deadletterqueue.context.headers.enable",
          "value": "true"
        },
        {
          "name": "consumer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "consumer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "consumer.override.max.poll.interval.ms",
          "value": "${max.poll.interval.ms}"
        },
        {
          "name": "consumer.override.max.poll.records",
          "value": "${max.poll.records}"
        },
        {
          "name": "producer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "admin.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "admin.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "errors.tolerance"
        },
        {
          "name": "errors.deadletterqueue.topic.name",
          "dynamic.mapper": {
            "name": "errors.deadletterqueue.topic.mapper"
          }
        }
      ]
    },
    {
      "template_id": "csfle-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "csfle.enabled",
          "type": "BOOLEAN",
          "default_value": "false",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 1,
          "docs_hidden": true,
          "display_name": "Enable Client-Side Field Level Encryption",
          "documentation": "Determines whether the connector honours CSFLE rules or not",
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "csfle.configs.visible",
                "values": "false"
              },
              "metadata": {
                "visibility": "false"
              }
            }
          ]
        },
        {
          "name": "csfle.onFailure",
          "type": "STRING",
          "required": false,
          "default_value": "ERROR",
          "importance": "MEDIUM",
          "group": "CSFLE",
          "order_in_group": 3,
          "docs_hidden": true,
          "display_name": "Connector behaviour on data decryption failure",
          "documentation": "Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.",
          "recommended_values": [
            "ERROR",
            "NONE"
          ]
        },
        {
          "name": "sr.service.account.id",
          "type": "STRING",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 2,
          "docs_hidden": true,
          "display_name": "Schema Registry Service Account",
          "documentation": "Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry."
        }
      ],
      "connector_configs": [
        {
          "name": "csfle.enabled"
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.onFailure": {
              "ERROR": "ERROR",
              "NONE": "NONE"
            }
          }
        },
        {
          "name": "value.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.onFailure": {
              "ERROR": "ERROR",
              "NONE": "NONE"
            }
          }
        },
        {
          "name": "key.converter.auto.register.schemas",
          "switch": {
            "csfle.enabled": {
              "true": "false"
            }
          }
        },
        {
          "name": "key.converter.use.latest.version",
          "switch": {
            "csfle.enabled": {
              "true": "true"
            }
          }
        },
        {
          "name": "key.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        },
        {
          "name": "value.converter.auto.register.schemas",
          "switch": {
            "csfle.enabled": {
              "true": "false"
            }
          }
        },
        {
          "name": "value.converter.use.latest.version",
          "switch": {
            "csfle.enabled": {
              "true": "true"
            }
          }
        }
      ]
    },
    {
      "template_id": "schema-registry",
      "abstract": true,
      "config_defs": [
        {
          "name": "schema.context.name",
          "type": "STRING",
          "group": "Schema Config",
          "order_in_group": 1,
          "importance": "MEDIUM",
          "display_name": "Schema context",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "default_value": "default",
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "schema.context.name",
          "type": "STRING",
          "group": "Schema Config",
          "order_in_group": 1,
          "importance": "MEDIUM",
          "display_name": "Schema context",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "default_value": "default",
          "dependents": [
            "schema.registry.url"
          ]
        }
      ],
      "connector_configs": []
    },
    {
      "template_id": "blob-store-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "topics.dir",
          "type": "STRING",
          "required": false,
          "default_value": "topics",
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 1,
          "display_name": "Topic directory",
          "documentation": "Top-level directory where ingested data is stored."
        },
        {
          "name": "path.format",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "default_value": "'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH",
          "order_in_group": 2,
          "display_name": "Path format",
          "documentation": "This configuration is used to set the format of the data directories when partitioning with TimeBasedPartitioner. The format set in this configuration converts the Unix timestamp to a valid directory string. To organize files like this example, filesystem://store-name/json_logs/daily/<Topic-Name>/dt=2020-02-06/hr=09/<files>, use the properties: topics.dir=json_logs/daily, path.format='dt'=YYYY-MM-dd/'hr'=HH, and time.interval=HOURLY.",
          "dependents": [
            "timestamp.field"
          ],
          "sanitizers": [
            {
              "name": "trim.blank.string"
            }
          ]
        },
        {
          "name": "time.interval",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 3,
          "display_name": "Time interval",
          "documentation": "Partitioning interval of data, according to the time ingested to storage.",
          "recommended_values": [
            "DAILY",
            "HOURLY"
          ]
        },
        {
          "name": "rotate.schedule.interval.ms",
          "type": "INT",
          "default_value": "-1",
          "importance": "MEDIUM",
          "group": "Organize my data by...",
          "order_in_group": 4,
          "documentation": "Scheduled rotation uses rotate.schedule.interval.ms to close the file and upload to storage on a regular basis using the current time, rather than the record time. Setting rotate.schedule.interval.ms is nondeterministic and will invalidate exactly-once guarantees. Minimum value is 600000ms (10 minutes).",
          "display_name": "Maximum span of record time (in ms) before scheduled rotation"
        },
        {
          "name": "rotate.interval.ms",
          "type": "INT",
          "default_value": "${time.interval}",
          "default_value_provider": {
            "name": "rotation.config.provider"
          },
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 5,
          "documentation": "The connector\u2019s rotation interval specifies the maximum timespan (in milliseconds) a file can remain open and ready for additional records. In other words, when using rotate.interval.ms, the timestamp for each file starts with the timestamp of the first record inserted in the file. The connector closes and uploads a file to the blob store when the next record's timestamp does not fit into the file's rotate.interval time span from the first record's timestamp. If the connector has no more records to process, the connector may keep the file open until the connector can process another record (which can be a long time). Minimum value is 600000ms (10 minutes).",
          "display_name": "Maximum span of record time (in ms) before rotation"
        },
        {
          "name": "flush.size",
          "type": "INT",
          "required": false,
          "default_value": "1000",
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 6,
          "display_name": "Flush size",
          "documentation": "Number of records written to storage before invoking file commits."
        },
        {
          "name": "timestamp.field",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "default_value": "",
          "order_in_group": 7,
          "display_name": "Timestamp field name",
          "documentation": "Sets the field that contains the timestamp used for the ``TimeBasedPartitioner``. If not set, it defaults to using the Kafka record timestamp. If a field name is specified, it extracts the timestamp from that field in the record value.",
          "dependents": [
            "path.format"
          ]
        },
        {
          "name": "timezone",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "default_value": "UTC",
          "order_in_group": 8,
          "display_name": "Timezone",
          "documentation": "Sets the timezone used by the TimeBasedPartitioner.",
          "recommender": {
            "name": "timezone"
          }
        },
        {
          "name": "value.converter.connect.meta.data",
          "type": "BOOLEAN",
          "required": false,
          "importance": "MEDIUM",
          "group": "Organize my data by...",
          "order_in_group": 11,
          "default_value": "true",
          "display_name": "Value Converter Connect Metadata",
          "documentation": "Toggle for enabling/disabling connect converter to add its meta data to the output schema or not.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "locale",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "default_value": "en",
          "order_in_group": 9,
          "display_name": "Locale",
          "documentation": "Sets the locale to use with TimeBasedPartitioner.",
          "recommender": {
            "name": "locale"
          }
        },
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        },
        {
          "name": "rotate.schedule.interval.ms",
          "type": "INT",
          "default_value": "-1",
          "importance": "MEDIUM",
          "group": "Organize my data by...",
          "order_in_group": 4,
          "documentation": "Scheduled rotation uses rotate.schedule.interval.ms to close the file and upload to storage on a regular basis using the current time, rather than the record time. Setting rotate.schedule.interval.ms is nondeterministic and will invalidate exactly-once guarantees.",
          "display_name": "Maximum span of record time (in ms) before scheduled rotation"
        },
        {
          "name": "rotate.interval.ms",
          "type": "INT",
          "default_value": "${time.interval}",
          "default_value_provider": {
            "name": "rotation.config.provider"
          },
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 5,
          "documentation": "The connector\u2019s rotation interval specifies the maximum timespan (in milliseconds) a file can remain open and ready for additional records. In other words, when using rotate.interval.ms, the timestamp for each file starts with the timestamp of the first record inserted in the file. The connector closes and uploads a file to the blob store when the next record's timestamp does not fit into the file's rotate.interval time span from the first record's timestamp. If the connector has no more records to process, the connector may keep the file open until the connector can process another record (which can be a long time).",
          "display_name": "Maximum span of record time (in ms) before rotation"
        },
        {
          "name": "timestamp.field",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "default_value": "",
          "order_in_group": 7,
          "display_name": "Timestamp field name",
          "documentation": "Sets the field that contains the timestamp used for the TimeBasedPartitioner",
          "dependents": [
            "path.format"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "partition.duration.ms",
          "switch": {
            "time.interval": {
              "DAILY": "86400000",
              "HOURLY": "3600000"
            }
          }
        },
        {
          "name": "flush.size"
        },
        {
          "name": "topics.dir"
        },
        {
          "name": "path.format"
        },
        {
          "name": "partitioner.class",
          "value": "io.confluent.connect.storage.partitioner.TimeBasedPartitioner"
        },
        {
          "name": "locale"
        },
        {
          "name": "timezone"
        },
        {
          "name": "timestamp.extractor",
          "switch": {
            "timestamp.field": {
              "": "Record",
              "DEFAULT": "RecordField"
            }
          }
        },
        {
          "name": "timestamp.field",
          "switch": {
            "timestamp.field": {
              "": "timestamp",
              "DEFAULT": "${timestamp.field}"
            }
          }
        },
        {
          "name": "rotate.interval.ms"
        },
        {
          "name": "rotate.schedule.interval.ms"
        },
        {
          "name": "value.converter.connect.meta.data"
        }
      ]
    },
    {
      "template_id": "input-data-format",
      "abstract": true,
      "config_defs": [
        {
          "name": "input.data.format",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "alias": "data.format",
          "group": "Input messages",
          "order_in_group": 1,
          "display_name": "Input Kafka record value format",
          "documentation": "Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON",
            "BYTES"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "input.data.format",
          "type": "STRING",
          "required": true,
          "default_value": "JSON",
          "importance": "HIGH",
          "alias": "data.format",
          "group": "Input messages",
          "order_in_group": 1,
          "display_name": "Input Kafka record value format",
          "documentation": "Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON",
            "BYTES"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "value.converter.schemas.enable",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "alias": "schemas.enable",
          "display_name": "value.converter.schemas.enable",
          "documentation": "Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.replace.null.with.default",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "alias": "replace.null.with.default",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.replace.null.with.default",
          "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.ignore.default.for.nullables",
          "alias": "ignore.default.for.nullables",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.ignore.default.for.nullables",
          "documentation": "When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters."
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "type": "BOOLEAN",
          "documentation": "Whether to scrub invalid names by replacing invalid characters with valid characters. Applicable for Avro and Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.scrub.invalid.names"
        }
      ],
      "connector_configs": [
        {
          "name": "value.converter",
          "switch": {
            "input.data.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "BYTES": "org.apache.kafka.connect.converters.ByteArrayConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter"
            }
          }
        },
        {
          "name": "value.converter.schemas.enable",
          "switch": {
            "input.data.format": {
              "JSON": false
            }
          }
        },
        {
          "name": "value.converter.schema.registry.url",
          "switch": {
            "input.data.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.credentials.source",
          "switch": {
            "input.data.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.user.info",
          "switch": {
            "input.data.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        },
        {
          "name": "value.converter.schemas.enable"
        },
        {
          "name": "value.converter.replace.null.with.default"
        },
        {
          "name": "value.converter.ignore.default.for.nullables"
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "dynamic.mapper": {
            "name": "value.converter.scrub.invalid.names.mapper"
          }
        }
      ]
    },
    {
      "template_id": "input-key-format",
      "abstract": true,
      "config_defs": [
        {
          "name": "input.key.format",
          "type": "STRING",
          "required": false,
          "default_value": "JSON",
          "importance": "HIGH",
          "group": "Input messages",
          "order_in_group": 2,
          "display_name": "Input Kafka record key format",
          "alias": "key.format",
          "documentation": "Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "BYTES",
            "JSON",
            "JSON_SR",
            "PROTOBUF",
            "STRING"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "key.converter.schemas.enable",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "key.converter.schemas.enable",
          "documentation": "Include schemas within each of the serialized keys. Input message keys must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Key Converter."
        },
        {
          "name": "key.converter.replace.null.with.default",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "key.converter.replace.null.with.default",
          "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Key Converter."
        }
      ],
      "connector_configs": [
        {
          "name": "key.converter",
          "switch": {
            "input.key.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "STRING": "org.apache.kafka.connect.storage.StringConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter",
              "BYTES": "org.apache.kafka.connect.converters.ByteArrayConverter"
            }
          }
        },
        {
          "name": "key.converter.schemas.enable",
          "switch": {
            "input.key.format": {
              "JSON": "false"
            }
          }
        },
        {
          "name": "key.converter.schema.registry.url",
          "switch": {
            "input.key.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.credentials.source",
          "switch": {
            "input.key.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.user.info",
          "switch": {
            "input.key.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        },
        {
          "name": "key.converter.schemas.enable"
        },
        {
          "name": "key.converter.replace.null.with.default"
        }
      ]
    },
    {
      "template_id": "gcs-s3-common-sink-params",
      "abstract": true,
      "config_defs": [
        {
          "name": "behavior.on.null.values",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 8,
          "display_name": "How to handle records with null values",
          "documentation": "How to handle records with null values, e.g Kafka tombstone records. Valid options are \u2018ignore\u2019 and \u2018fail\u2019. Default is \u2018ignore\u2019",
          "default_value": "ignore",
          "recommended_values": [
            "ignore",
            "fail"
          ]
        },
        {
          "name": "enhanced.avro.schema.support",
          "type": "BOOLEAN",
          "required": false,
          "importance": "LOW",
          "group": "Organize my data by...",
          "order_in_group": 9,
          "default_value": "true",
          "display_name": "Preserves Avro schema information. True by default",
          "documentation": "When set to true, this property preserves Avro schema package information and Enums when going from Avro schema to Connect schema. This information is added back in when going from Connect schema to Avro schema.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "schema.compatibility",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Organize my data by...",
          "order_in_group": 10,
          "default_value": "NONE",
          "display_name": "Schema Compatibility",
          "documentation": "The schema compatibility rule to use when the connector is observing schema changes.",
          "recommended_values": [
            "NONE",
            "BACKWARD",
            "FORWARD",
            "FULL"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "behavior.on.null.values"
        },
        {
          "name": "enhanced.avro.schema.support"
        },
        {
          "name": "schema.compatibility"
        }
      ]
    },
    {
      "template_id": "aws-authentication",
      "abstract": true,
      "config_defs": [
        {
          "name": "authentication.method",
          "type": "STRING",
          "required": false,
          "default_value": "Access Keys",
          "importance": "HIGH",
          "group": "AWS credentials",
          "order_in_group": 1,
          "display_name": "Authentication method",
          "documentation": "Select how you want to authenticate with AWS.",
          "recommended_values": [
            "IAM Roles",
            "Access Keys"
          ],
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "provider.integration.visible",
                "values": "false"
              },
              "metadata": {
                "visibility": "false"
              }
            }
          ]
        },
        {
          "name": "provider.integration.id",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "AWS credentials",
          "order_in_group": 2,
          "display_name": "Provider Integration",
          "documentation": "Select an existing integration that has access to your resource. In case you need to integrate a new IAM role, use provider integration"
        }
      ],
      "connector_configs": [
        {
          "name": "authentication.method",
          "switch": {
            "authentication.method": {
              "IAM Roles": "IAM Roles",
              "Access Keys": "Access Keys"
            }
          }
        },
        {
          "name": "provider.integration.id"
        },
        {
          "name": "customer.aws.iam.role.arn"
        },
        {
          "name": "external.id"
        },
        {
          "name": "middleware.external.id"
        },
        {
          "name": "confluent.aws.iam.role.arn"
        },
        {
          "name": "aws.iam.assume.role.session.name"
        },
        {
          "name": "provider.integration.max.retries"
        },
        {
          "name": "connect.aws.iam.role.validation.disable"
        }
      ]
    },
    {
      "template_id": "super",
      "abstract": true,
      "config_defs": [
        {
          "name": "auto.restart.on.user.error",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "MEDIUM",
          "group": "Auto-restart policy",
          "order_in_group": 1,
          "display_name": "Enable Connector Auto-restart",
          "documentation": "Enable connector to automatically restart on user-actionable errors."
        },
        {
          "name": "value.converter.enhanced.avro.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information and Enums. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data",
          "type": "BOOLEAN",
          "documentation": "Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to generate an index suffix for unions. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums",
          "type": "BOOLEAN",
          "documentation": "Whether to represent enums as integers. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should be specified with an optional label. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls",
          "type": "BOOLEAN",
          "documentation": "Whether to generate a struct variable for null values. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should use primitive wrapper messages. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives",
          "type": "BOOLEAN",
          "documentation": "Whether a wrapper message should be interpreted as a raw primitive at root level. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties",
          "type": "BOOLEAN",
          "documentation": "Whether to allow additional properties for object schemas. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired",
          "type": "BOOLEAN",
          "documentation": "Whether to set non-required properties to be optional. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format",
          "type": "STRING",
          "recommended_values": [
            "BASE64",
            "NUMERIC"
          ],
          "documentation": "Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:\nBASE64 to serialize DECIMAL logical types as base64 encoded binary data and\nNUMERIC to serialize Connect DECIMAL logical type values in JSON/JSON_SR as a number representing the decimal value.",
          "group": "Additional Configs",
          "alias": "json.output.decimal.format",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.decimal.format",
          "default_value": "BASE64"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "type": "BOOLEAN",
          "documentation": "Specify if the Serializer should attempt to register the Schema.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.auto.register.schemas"
        },
        {
          "name": "value.converter.use.latest.version",
          "type": "BOOLEAN",
          "documentation": "Use latest version of schema in subject for serialization when auto.register.schemas is false.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.latest.version"
        },
        {
          "name": "value.converter.latest.compatibility.strict",
          "type": "BOOLEAN",
          "documentation": "Verify latest subject version is backward compatible when `use.latest.version` is `true`.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "type": "STRING",
          "default_value": "TopicNameStrategy",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "alias": "key.subject.name.strategy",
          "documentation": "How to construct the subject name for key schema registration.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "key.converter.key.subject.name.strategy"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "default_value": "TopicNameStrategy",
          "alias": "subject.name.strategy,value.subject.name.strategy",
          "documentation": "Determines how to construct the subject name under which the value schema is registered with Schema Registry.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.value.subject.name.strategy"
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "DefaultReferenceSubjectNameStrategy",
            "QualifiedReferenceSubjectNameStrategy"
          ],
          "default_value": "DefaultReferenceSubjectNameStrategy",
          "documentation": "Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.reference.subject.name.strategy"
        },
        {
          "name": "value.converter.allow.optional.map.keys",
          "type": "BOOLEAN",
          "documentation": "Allow optional string map key when converting from Connect Schema to Avro Schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions",
          "type": "BOOLEAN",
          "default_value": "false",
          "documentation": "Whether to flatten singleton unions. Applicable for Avro and JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2",
          "type": "BOOLEAN",
          "documentation": "Whether proto2 optionals are supported. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to flatten unions (oneofs). Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "header.converter",
          "documentation": "The converter class for the headers. This is used to serialize and deserialize the headers of the messages.",
          "recommended_values": [
            "org.apache.kafka.connect.storage.SimpleHeaderConverter",
            "org.apache.kafka.connect.storage.StringConverter",
            "org.apache.kafka.connect.json.JsonConverter",
            "org.apache.kafka.connect.converters.BooleanConverter",
            "org.apache.kafka.connect.converters.DoubleConverter",
            "org.apache.kafka.connect.converters.FloatConverter",
            "org.apache.kafka.connect.converters.IntegerConverter",
            "org.apache.kafka.connect.converters.LongConverter",
            "org.apache.kafka.connect.converters.ShortConverter"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "auto.restart.on.user.error"
        },
        {
          "name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "dynamic.mapper": {
            "name": "value.converter.auto.register.schemas.mapper"
          }
        },
        {
          "name": "value.converter.use.latest.version",
          "dynamic.mapper": {
            "name": "value.converter.use.latest.version.mapper"
          }
        },
        {
          "name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.reference.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter"
        }
      ]
    },
    {
      "template_id": "super-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "consumer.override.auto.offset.reset",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "consumer.override.auto.offset.reset",
          "documentation": "Defines the behavior of the consumer when there is no committed position (which occurs when the group is first initialized) or when an offset is out of range. You can choose either to reset the position to the \u201cearliest\u201d offset or the \u201clatest\u201d offset (the default). You can also select \u201cnone\u201d if you would rather set the initial offset yourself and you are willing to handle out of range errors manually. More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#auto-offset-reset",
          "recommended_values": [
            "earliest",
            "latest",
            "none"
          ]
        },
        {
          "name": "consumer.override.isolation.level",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "consumer.override.isolation.level",
          "documentation": "Controls how to read messages written transactionally. If set to read_committed, consumer.poll() will only return transactional messages which have been committed. If set to read_uncommitted (the default), consumer.poll() will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode.  More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#isolation-level",
          "recommended_values": [
            "read_committed",
            "read_uncommitted"
          ]
        },
        {
          "name": "topics.regex",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Which topics do you want to get data from?",
          "display_name": "Topics Regex",
          "documentation": "A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually."
        }
      ],
      "connector_configs": [
        {
          "name": "consumer.override.auto.offset.reset"
        },
        {
          "name": "consumer.override.isolation.level"
        },
        {
          "name": "topics.regex"
        }
      ]
    }
  ]
}