{
  "templates": [
    {
      "template_id": "BigQueryStorageSink",
      "connector_type": "SINK",
      "connector.class": "io.confluent.connect.bigquerystorage.BigQueryStorageSinkConnector",
      "config_defs": [
        {
          "name": "topics"
        },
        {
          "name": "authentication.method",
          "type": "STRING",
          "required": false,
          "default_value": "Google cloud service account",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 1,
          "display_name": "Authentication method",
          "documentation": "Select how you want to authenticate with BigQuery.",
          "recommended_values": [
            "Google cloud service account",
            "OAuth 2.0 (Shared app)",
            "OAuth 2.0 (Bring your own app)"
          ]
        },
        {
          "name": "keyfile",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 2,
          "display_name": "GCP credentials file",
          "documentation": "GCP service account JSON file with write permissions for BigQuery.",
          "sanitizers": [
            {
              "name": "gcp.credentials.sanitize"
            }
          ]
        },
        {
          "name": "oauth.client.id",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 3,
          "display_name": "Client ID",
          "documentation": "Client ID of your Google OAuth application."
        },
        {
          "name": "oauth.client.secret",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 4,
          "display_name": "Client secret",
          "documentation": "Client secret of your Google OAuth application."
        },
        {
          "name": "oauth.auth.button",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 5,
          "display_name": "Connect with Google Cloud",
          "documentation": "Initiates the OAuth authentication flow."
        },
        {
          "name": "oauth.refresh.token",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 6,
          "display_name": "Refresh Token",
          "documentation": "OAuth 2.0 refresh token for BigQuery."
        },
        {
          "name": "oauth.auth.endpoint",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "https://accounts.google.com/o/oauth2/v2/auth",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 7,
          "display_name": "Auth Endpoint",
          "documentation": "GCP OAuth 2.0 Auth Endpoint."
        },
        {
          "name": "oauth.token.endpoint",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "https://www.googleapis.com/oauth2/v4/token",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 8,
          "display_name": "Token Endpoint",
          "documentation": "GCP OAuth 2.0 Token Endpoint."
        },
        {
          "name": "oauth.client.auth.mode",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "REQUEST_BODY",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 9,
          "display_name": "Client authorization mode",
          "documentation": "Client authorization mode i.e. via RequestBody or Basic Auth Header."
        },
        {
          "name": "oauth.scopes",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "https://www.googleapis.com/auth/bigquery",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 10,
          "display_name": "BigQuery OAuth 2.0 Scopes",
          "documentation": "BigQuery Storage Write API OAuth 2.0 Scopes."
        },
        {
          "name": "oauth.query.params",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "access_type:offline,prompt:consent",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 11,
          "display_name": "BigQuery OAuth query parameters",
          "documentation": "BigQuery OAuth query parameters."
        },
        {
          "name": "oauth.session.id",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 12,
          "display_name": "Connect OAuth Session ID",
          "documentation": "Connect OAuth Session ID."
        },
        {
          "name": "oauth.use.shared.app",
          "type": "BOOLEAN",
          "required": false,
          "queryable_internal": true,
          "default_value": false,
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 13,
          "display_name": "User shared app",
          "documentation": "Whether to use shared app."
        },
        {
          "name": "oauth.config.map",
          "type": "STRING",
          "required": false,
          "queryable_internal": true,
          "default_value": "auth_endpoint:oauth.auth.endpoint,token_endpoint:oauth.token.endpoint,client_id:oauth.client.id,client_secret:oauth.client.secret,client_auth_mode:oauth.client.auth.mode,scopes:oauth.scopes,query_params:oauth.query.params,oauth_session_id:oauth.session.id,refresh_token:oauth.refresh.token,connector_class:connector.class,auth_button:oauth.auth.button,use_shared_app:oauth.use.shared.app",
          "importance": "HIGH",
          "group": "GCP credentials",
          "order_in_group": 14,
          "display_name": "Connect OAuth Config Map",
          "documentation": "Connect OAuth Config Map to map OAuth API parameters to ConfigDefs."
        },
        {
          "name": "project",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "BigQuery details",
          "order_in_group": 1,
          "display_name": "Project ID",
          "documentation": "ID for the GCP project where BigQuery is located."
        },
        {
          "name": "datasets",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "BigQuery details",
          "order_in_group": 2,
          "display_name": "Dataset",
          "documentation": "Name of the BigQuery dataset where table(s) is located.",
          "dependents": [
            "project"
          ]
        },
        {
          "name": "ingestion.mode",
          "type": "STRING",
          "required": true,
          "default_value": "STREAMING",
          "importance": "HIGH",
          "group": "Ingestion Mode details",
          "order_in_group": 1,
          "display_name": "Ingestion Mode",
          "documentation": "Select a mode to ingest data into the table. Select STREAMING for reduced latency. Select BATCH LOADING for cost savings. Select UPSERT for upserting records. Select UPSERT_DELETE for upserting and deleting records.",
          "recommended_values": [
            "STREAMING",
            "BATCH LOADING",
            "UPSERT",
            "UPSERT_DELETE"
          ]
        },
        {
          "name": "input.data.format",
          "required": true,
          "order_in_group": 2,
          "display_name": "Input Kafka record value format",
          "documentation": "Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, and JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, or PROTOBUF.",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON"
          ]
        },
        {
          "name": "input.key.format",
          "order_in_group": 1,
          "display_name": "Input Kafka record key format",
          "default_value": "BYTES",
          "documentation": "Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "BYTES",
            "JSON",
            "JSON_SR",
            "PROTOBUF",
            "STRING"
          ]
        },
        {
          "name": "commit.interval",
          "type": "INT",
          "required": false,
          "default_value": "60",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 1,
          "display_name": "Commit Interval",
          "documentation": "The interval, in seconds, the connector attempts to commit streamed records. Set the interval between 60 seconds (1 minute) and 14,400 seconds (4 hours). Be careful when setting the commit interval as on every commit interval, a task calls the ``CreateWriteStream`` API which is subject to a `quota <https://cloud.google.com/bigquery/quotas#write-api-limits:~:text=handle%20unexpected%20demand.-,CreateWriteStream,-requests>`. For example, if you have five tasks (may belong to different connectors also) set to commit every 60 seconds to a project, there will be five calls to ``CreateWriteStream`` API on that project every minute. If the count exceeds the allowed quota, some tasks may fail."
        },
        {
          "name": "topic2table.map",
          "type": "STRING",
          "required": false,
          "importance": "MEDIUM",
          "default_value": "",
          "group": "Insertion and DDL support",
          "order_in_group": 2,
          "display_name": "Kafka Topic to BigQuery Table Map",
          "documentation": "Map of topics to tables (optional). The required format is comma-separated tuples. For example, <topic-1>:<table-1>,<topic-2>:<table-2>,... Note that a topic name must not be modified using a regex SMT while using this option. If this property is used, ``sanitize.topics`` is ignored. Also, if the topic-to-table map doesn't contain the topic for a record, the connector creates a table with the same name as the topic name."
        },
        {
          "name": "sanitize.topics",
          "type": "STRING",
          "required": false,
          "default_value": "true",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 3,
          "display_name": "Sanitize topics",
          "documentation": "Designates whether to automatically sanitize topic names before using them as table names in BigQuery. If not enabled, topic names are used as table names.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "sanitize.field.names",
          "type": "STRING",
          "required": false,
          "default_value": "false",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 4,
          "display_name": "Sanitize field names",
          "documentation": "Whether to automatically sanitize field names before using them as field names in BigQuery. BigQuery specifies that field names can only contain letters, numbers, and underscores. The sanitizer replaces invalid symbols with underscores. If the field name starts with a digit, the sanitizer adds an underscore in front of field name. Caution: Key duplication errors can occur if different fields are named a.b and a_b, for instance. After being sanitized, field names a.b and a_b will have same value.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "sanitize.field.names.in.array",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "MEDIUM",
          "group": "Insertion and DDL support",
          "order_in_group": 5,
          "display_name": "Sanitize array fields",
          "documentation": "Whether to automatically sanitize field names inside arrays. When enabled, field names inside arrays will also be sanitized according to BigQuery naming rules. This setting only takes effect if 'sanitize.field.names' is also enabled.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "auto.update.schemas",
          "type": "STRING",
          "required": false,
          "default_value": "DISABLED",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 5,
          "display_name": "Auto update schemas",
          "documentation": "Designates whether or not to automatically update BigQuery schemas. New fields in record schemas must be nullable. Note: Supports AVRO, JSON_SR, and PROTOBUF message format only.",
          "dependents": [
            "input.data.format"
          ],
          "recommended_values": [
            "DISABLED",
            "ADD NEW FIELDS"
          ]
        },
        {
          "name": "auto.create.tables",
          "type": "STRING",
          "required": false,
          "default_value": "DISABLED",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 5,
          "display_name": "Auto create tables",
          "documentation": "Designates whether or not to automatically create BigQuery tables. Note: Supports AVRO, JSON_SR, and PROTOBUF message format only.",
          "dependents": [
            "input.data.format"
          ],
          "recommended_values": [
            "DISABLED",
            "NON-PARTITIONED",
            "PARTITION by INGESTION TIME",
            "PARTITION by FIELD"
          ]
        },
        {
          "name": "partitioning.type",
          "type": "STRING",
          "required": false,
          "default_value": "DAY",
          "importance": "LOW",
          "group": "Insertion and DDL support",
          "order_in_group": 6,
          "display_name": "Partitioning type",
          "documentation": "The time partitioning type to use when creating new partitioned tables. Existing tables will not be altered to use this partitioning type.",
          "recommended_values": [
            "HOUR",
            "DAY",
            "MONTH",
            "YEAR"
          ]
        },
        {
          "name": "timestamp.partition.field.name",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Insertion and DDL support",
          "order_in_group": 7,
          "display_name": "Timestamp partition field name",
          "documentation": "The name of the field in the value that contains the timestamp to partition by in BigQuery. This also enables timestamp partitioning for each table."
        },
        {
          "name": "use.date.time.formatter",
          "type": "STRING",
          "required": false,
          "default_value": "false",
          "importance": "HIGH",
          "group": "Insertion and DDL support",
          "order_in_group": 8,
          "display_name": "Use Date Time Formatter",
          "documentation": "Specify whether to use a ``DateTimeFormatter`` to support a wide range of epochs. Setting this true will use ``DateTimeFormatter`` over default ``SimpleDateFormat``. The output might vary for same input between the two formatters.",
          "recommended_values": [
            "true",
            "false"
          ]
        },
        {
          "name": "topic2clustering.fields.map",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "default_value": "",
          "group": "Insertion and DDL support",
          "order_in_group": 9,
          "display_name": "Kafka Topic to Clustering Fields Map",
          "documentation": "Maps topics to their corresponding table clustering fields (optional). Format: comma-separated tuples, e.g., topic1:[col1|col2], topic2:[col1|col2|..], ... Specifies the fields used to cluster data in BigQuery. The order of the fields determines the clustering precedence."
        },
        {
          "name": "use.integer.for.int8.int16",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Insertion and DDL support",
          "order_in_group": 10,
          "display_name": "Use INTEGER for INT8 and INT16",
          "documentation": "Determines how the connector stores INT8 (BYTE) and INT16 (SHORT) data types in BigQuery during auto table creation and schema update. When set to ``false``(default), these values are stored as FLOAT. When set to ``true``, INT8 (BYTE) and INT16 (SHORT) values are stored as INTEGER.",
          "recommended_values": [
            "true",
            "false"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "authentication.method",
          "switch": {
            "authentication.method": {
              "Google cloud service account": "Service Account",
              "OAuth 2.0 (Shared app)": "OAuth 2.0",
              "OAuth 2.0 (Bring your own app)": "OAuth 2.0"
            }
          }
        },
        {
          "name": "project"
        },
        {
          "name": "defaultDataset",
          "value": "${datasets}"
        },
        {
          "name": "connector.endpoint",
          "value": "https://www.googleapis.com"
        },
        {
          "name": "keyfile"
        },
        {
          "name": "keySource",
          "value": "JSON"
        },
        {
          "name": "oauth.client.id"
        },
        {
          "name": "oauth.client.secret",
          "value": "${file:/mnt/secrets/connect-external-secrets-{{.logicalClusterId}}.properties:oauth.client.secret}"
        },
        {
          "name": "oauth.refresh.token",
          "value": "${file:/mnt/secrets/connect-external-secrets-{{.logicalClusterId}}.properties:oauth.refresh.token}"
        },
        {
          "name": "transforms",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "",
              "UPSERT_DELETE": "",
              "DEFAULT": "requireMapTransform"
            }
          }
        },
        {
          "name": "transforms.requireMapTransform.type",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "",
              "UPSERT_DELETE": "",
              "DEFAULT": "io.confluent.cctransforms.RequireMapTransform"
            }
          }
        },
        {
          "name": "autoCreateTables",
          "switch": {
            "auto.create.tables": {
              "DISABLED": "false",
              "DEFAULT": "true"
            }
          }
        },
        {
          "name": "enableBatchMode",
          "switch": {
            "ingestion.mode": {
              "BATCH LOADING": "true",
              "DEFAULT": "false"
            }
          }
        },
        {
          "name": "cdc.mode",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "UPSERT",
              "UPSERT_DELETE": "UPSERT_DELETE",
              "DEFAULT": "DISABLED"
            }
          }
        },
        {
          "name": "add.record.key",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "true",
              "UPSERT_DELETE": "true",
              "DEFAULT": "false"
            }
          }
        },
        {
          "name": "primary.key",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "RECORD_KEY",
              "UPSERT_DELETE": "RECORD_KEY",
              "DEFAULT": "DISABLED"
            }
          }
        },
        {
          "name": "threadPoolSize",
          "switch": {
            "ingestion.mode": {
              "UPSERT": "1",
              "UPSERT_DELETE": "1",
              "DEFAULT": "10"
            }
          }
        },
        {
          "name": "isDedicatedCluster",
          "value": "${kafka.dedicated}"
        },
        {
          "name": "queueSize",
          "switch": {
            "ingestion.mode": {
              "BATCH LOADING": "-1",
              "DEFAULT": "100"
            }
          }
        },
        {
          "name": "commitInterval",
          "value": "${commit.interval}"
        },
        {
          "name": "allowNewBigQueryFields",
          "switch": {
            "auto.update.schemas": {
              "ADD NEW FIELDS": "true",
              "DEFAULT": "false"
            }
          }
        },
        {
          "name": "sanitizeTopics",
          "value": "${sanitize.topics}"
        },
        {
          "name": "sanitizeFieldNames",
          "value": "${sanitize.field.names}"
        },
        {
          "name": "sanitizeFieldNamesInArray",
          "value": "${sanitize.field.names.in.array}"
        },
        {
          "name": "timestampPartitionFieldName",
          "switch": {
            "auto.create.tables": {
              "PARTITION by FIELD": "${timestamp.partition.field.name}",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "timePartitioningType",
          "switch": {
            "auto.create.tables": {
              "DISABLED": "NONE",
              "NON-PARTITIONED": "NONE",
              "DEFAULT": "${partitioning.type}"
            }
          }
        },
        {
          "name": "runtimeProvider",
          "value": "Confluent Cloud"
        },
        {
          "name": "topic2TableMap",
          "value": "${topic2table.map}"
        },
        {
          "name": "bigQueryRetry",
          "value": "10"
        },
        {
          "name": "use.date.time.formatter",
          "value": "${use.date.time.formatter}"
        },
        {
          "name": "topic2ClusteringFieldsMap",
          "value": "${topic2clustering.fields.map}"
        },
        {
          "name": "use.integer.for.int8.int16",
          "value": "${use.integer.for.int8.int16}"
        },
        {
          "name": "enableChangeSequenceNumber",
          "value": "false"
        }
      ]
    },
    {
      "template_id": "common",
      "global_validators": [
        {
          "name": "required",
          "priority": "HIGHEST"
        },
        {
          "name": "recommended.values",
          "priority": "HIGHER"
        }
      ],
      "abstract": true,
      "config_defs": [
        {
          "name": "connector.class",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 1,
          "display_name": "Connector class"
        },
        {
          "name": "name",
          "type": "STRING",
          "required": true,
          "importance": "HIGH",
          "group": "How should we connect to your data?",
          "order_in_group": 2,
          "display_name": "Connector name",
          "documentation": "Sets a name for your connector."
        },
        {
          "name": "tasks.max",
          "type": "INT",
          "required": true,
          "importance": "HIGH",
          "group": "Number of tasks for this connector",
          "order_in_group": 1,
          "display_name": "Tasks",
          "documentation": "Maximum number of tasks for the connector."
        },
        {
          "name": "kafka.auth.mode",
          "type": "STRING",
          "required": false,
          "default_value": "KAFKA_API_KEY",
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 1,
          "display_name": "Kafka Cluster Authentication mode",
          "documentation": "Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.",
          "recommended_values": [
            "SERVICE_ACCOUNT",
            "KAFKA_API_KEY"
          ]
        },
        {
          "name": "kafka.api.key",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka API Key",
          "documentation": "Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY."
        }
      ],
      "connector_configs": [
        {
          "name": "tasks.max"
        },
        {
          "name": "confluent.topic.bootstrap.servers",
          "value": "Placeholder value to pass connector validations"
        },
        {
          "name": "errors.log.enable",
          "value": "true"
        },
        {
          "name": "errors.log.include.messages",
          "value": "false"
        },
        {
          "name": "errors.retry.timeout",
          "value": "300000"
        },
        {
          "name": "errors.retry.delay.max.ms",
          "value": "30000"
        },
        {
          "name": "value.converter.ignore.modern.dialects",
          "value": "true"
        }
      ]
    },
    {
      "template_id": "common-kafka-connectivity",
      "abstract": true,
      "config_defs": [],
      "connector_configs": [
        {
          "name": "consumer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "producer.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.bootstrap.servers",
          "switch": {
            "connect.metadata_property.kafka.itsl.bootstrap.servers": {
              "UNSET": "${kafka.endpoint}",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.bootstrap.servers}"
            }
          }
        },
        {
          "name": "admin.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "producer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "consumer.override.ssl.trustmanager.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "SECURED": "ConfluentTls",
              "DEFAULT": "PKIX"
            }
          }
        },
        {
          "name": "admin.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "producer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "consumer.override.ssl.endpoint.identification.algorithm",
          "switch": {
            "connect.metadata_property.kafka.itsl.ssl.endpoint.identification.algorithm": {
              "UNSECURED_PREPROD_ONLY": "",
              "SECURED": "",
              "DEFAULT": "https"
            }
          }
        },
        {
          "name": "admin.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.security.providers",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "io.confluent.kafka.security.fips.provider.BcFipsProviderCreator,io.confluent.kafka.security.fips.provider.BcFipsJsseProviderCreator,io.confluent.kafka.server.plugins.ssl.ConfluentTrustProviderCreator",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.provider",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "BCJSSE",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.cipher.suites",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CCM,TLS_ECDHE_ECDSA_WITH_AES_128_CCM,TLS_ECDHE_ECDSA_WITH_AES_256_CCM_8,TLS_ECDHE_ECDSA_WITH_AES_128_CCM_8,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_AES_256_GCM_SHA384,TLS_AES_128_GCM_SHA256,TLS_AES_128_CCM_SHA256,TLS_AES_128_CCM_8_SHA256",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "admin.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "consumer.override.ssl.enabled.protocols",
          "switch": {
            "connect.fips.provider": {
              "BCJSSE": "TLSv1.2,TLSv1.3",
              "DEFAULT": null
            }
          }
        },
        {
          "name": "producer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "consumer.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "admin.override.confluent.lkc.id",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "",
              "DEFAULT": "${connect.metadata_property.kafka.itsl.embed.lkc}"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "producer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "consumer.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.mode",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "PROXY",
              "DEFAULT": "LOCAL"
            }
          }
        },
        {
          "name": "admin.override.confluent.proxy.protocol.client.version",
          "switch": {
            "connect.metadata_property.kafka.itsl.embed.lkc": {
              "SKIP": "NONE",
              "DEFAULT": "V2"
            }
          }
        }
      ]
    },
    {
      "template_id": "common-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "kafka.service.account.id",
          "type": "STRING",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 2,
          "display_name": "Kafka Service Account",
          "documentation": "The Service Account that will be used to generate the API keys to communicate with Kafka Cluster."
        },
        {
          "name": "kafka.api.secret",
          "type": "PASSWORD",
          "required": false,
          "importance": "HIGH",
          "group": "Kafka Cluster credentials",
          "order_in_group": 3,
          "display_name": "Kafka API Secret",
          "documentation": "Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.",
          "dependents": [
            "kafka.api.key"
          ]
        },
        {
          "name": "topics",
          "type": "LIST",
          "required": true,
          "importance": "HIGH",
          "group": "Which topics do you want to get data from?",
          "order_in_group": 1,
          "display_name": "Topic names",
          "documentation": "Identifies the topic name or a comma-separated list of topic names.",
          "dependents": [
            "kafka.api.secret"
          ],
          "sanitizers": [
            {
              "name": "trim.list"
            }
          ]
        },
        {
          "name": "max.poll.interval.ms",
          "type": "LONG",
          "required": false,
          "importance": "LOW",
          "group": "Consumer configuration",
          "order_in_group": 1,
          "display_name": "Max poll interval(ms)",
          "default_value": "300000",
          "documentation": "The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes)."
        },
        {
          "name": "max.poll.records",
          "type": "LONG",
          "required": false,
          "importance": "LOW",
          "group": "Consumer configuration",
          "order_in_group": 2,
          "display_name": "Max poll records",
          "default_value": "500",
          "documentation": "The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records."
        },
        {
          "name": "errors.tolerance",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "default_value": "all",
          "display_name": "errors.tolerance",
          "documentation": "Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.",
          "recommended_values": [
            "none",
            "all"
          ]
        },
        {
          "name": "errors.deadletterqueue.topic.name",
          "type": "STRING",
          "importance": "LOW",
          "group": "Which topics do you want to get data from?",
          "order_in_group": 2,
          "display_name": "Dead Letter Queue Topic Name",
          "documentation": "The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ``${connector}`` in the value to use it as a placeholder for the logical cluster ID.",
          "default_value": "dlq-${connector}"
        }
      ],
      "connector_configs": [
        {
          "name": "topics"
        },
        {
          "name": "errors.tolerance"
        },
        {
          "name": "errors.deadletterqueue.topic.name",
          "dynamic.mapper": {
            "name": "errors.deadletterqueue.topic.mapper"
          }
        },
        {
          "name": "errors.deadletterqueue.topic.replication.factor",
          "value": "3"
        },
        {
          "name": "errors.deadletterqueue.context.headers.enable",
          "value": "true"
        },
        {
          "name": "consumer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "consumer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "consumer.override.max.poll.interval.ms",
          "value": "${max.poll.interval.ms}"
        },
        {
          "name": "consumer.override.max.poll.records",
          "value": "${max.poll.records}"
        },
        {
          "name": "producer.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "producer.override.sasl.mechanism",
          "value": "PLAIN"
        },
        {
          "name": "admin.override.security.protocol",
          "value": "SASL_SSL"
        },
        {
          "name": "admin.override.sasl.mechanism",
          "value": "PLAIN"
        }
      ]
    },
    {
      "template_id": "csfle-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "csfle.enabled",
          "type": "BOOLEAN",
          "default_value": "false",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 1,
          "docs_hidden": true,
          "display_name": "Enable Client-Side Field Level Encryption",
          "documentation": "Determines whether the connector honours CSFLE rules or not",
          "conditional_metadata_provider": [
            {
              "name": "metadata.conditional.visible",
              "arguments": {
                "config": "csfle.configs.visible",
                "values": "false"
              },
              "metadata": {
                "visibility": "false"
              }
            }
          ]
        },
        {
          "name": "csfle.onFailure",
          "type": "STRING",
          "required": false,
          "default_value": "ERROR",
          "importance": "MEDIUM",
          "group": "CSFLE",
          "order_in_group": 3,
          "docs_hidden": true,
          "display_name": "Connector behaviour on data decryption failure",
          "documentation": "Configures the behavior for decryption failures. If set to ERROR, the connector will behave as configured for error behaviour. If set to NONE, the connector will ignore the decryption failure and proceed to write the data in its encrypted form.",
          "recommended_values": [
            "ERROR",
            "NONE"
          ]
        },
        {
          "name": "sr.service.account.id",
          "type": "STRING",
          "importance": "HIGH",
          "group": "CSFLE",
          "order_in_group": 2,
          "docs_hidden": true,
          "display_name": "Schema Registry Service Account",
          "documentation": "Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry."
        }
      ],
      "connector_configs": [
        {
          "name": "csfle.enabled"
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "value.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.onFailure": {
              "ERROR": "ERROR",
              "NONE": "NONE"
            }
          }
        },
        {
          "name": "value.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.disabled",
          "switch": {
            "csfle.enabled": {
              "true": "false",
              "false": "true"
            }
          }
        },
        {
          "name": "key.converter.rule.executors._ENCRYPT_.onFailure",
          "switch": {
            "csfle.onFailure": {
              "ERROR": "ERROR",
              "NONE": "NONE"
            }
          }
        },
        {
          "name": "key.converter.auto.register.schemas",
          "switch": {
            "csfle.enabled": {
              "true": "false"
            }
          }
        },
        {
          "name": "key.converter.use.latest.version",
          "switch": {
            "csfle.enabled": {
              "true": "true"
            }
          }
        },
        {
          "name": "key.converter.latest.cache.ttl.sec",
          "switch": {
            "csfle.enabled": {
              "true": "300"
            }
          }
        }
      ]
    },
    {
      "template_id": "schema-registry",
      "abstract": true,
      "config_defs": [
        {
          "name": "schema.context.name",
          "type": "STRING",
          "group": "Schema Config",
          "order_in_group": 1,
          "importance": "MEDIUM",
          "display_name": "Schema context",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "default_value": "default",
          "dependents": [
            "schema.registry.url"
          ]
        }
      ],
      "connector_configs": []
    },
    {
      "template_id": "input-data-format",
      "abstract": true,
      "config_defs": [
        {
          "name": "input.data.format",
          "type": "STRING",
          "required": true,
          "default_value": "JSON",
          "importance": "HIGH",
          "alias": "data.format",
          "group": "Input messages",
          "order_in_group": 1,
          "display_name": "Input Kafka record value format",
          "documentation": "Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON or BYTES. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.",
          "recommended_values": [
            "AVRO",
            "JSON_SR",
            "PROTOBUF",
            "JSON",
            "BYTES"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "value.converter.schemas.enable",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "alias": "schemas.enable",
          "display_name": "value.converter.schemas.enable",
          "documentation": "Include schemas within each of the serialized values. Input messages must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.replace.null.with.default",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "alias": "replace.null.with.default",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.replace.null.with.default",
          "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter."
        },
        {
          "name": "value.converter.ignore.default.for.nullables",
          "alias": "ignore.default.for.nullables",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "value.converter.ignore.default.for.nullables",
          "documentation": "When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters."
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "type": "BOOLEAN",
          "documentation": "Whether to scrub invalid names by replacing invalid characters with valid characters. Applicable for Avro and Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.scrub.invalid.names"
        }
      ],
      "connector_configs": [
        {
          "name": "value.converter",
          "switch": {
            "input.data.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "BYTES": "org.apache.kafka.connect.converters.ByteArrayConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter"
            }
          }
        },
        {
          "name": "value.converter.schemas.enable"
        },
        {
          "name": "value.converter.replace.null.with.default"
        },
        {
          "name": "value.converter.schema.registry.url",
          "switch": {
            "input.data.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.credentials.source",
          "switch": {
            "input.data.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "value.converter.basic.auth.user.info",
          "switch": {
            "input.data.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        },
        {
          "name": "value.converter.ignore.default.for.nullables"
        },
        {
          "name": "value.converter.scrub.invalid.names",
          "dynamic.mapper": {
            "name": "value.converter.scrub.invalid.names.mapper"
          }
        }
      ]
    },
    {
      "template_id": "input-key-format",
      "abstract": true,
      "config_defs": [
        {
          "name": "input.key.format",
          "type": "STRING",
          "required": false,
          "default_value": "JSON",
          "importance": "HIGH",
          "group": "Input messages",
          "order_in_group": 2,
          "display_name": "Input Kafka record key format",
          "alias": "key.format",
          "documentation": "Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "recommended_values": [
            "AVRO",
            "BYTES",
            "JSON",
            "JSON_SR",
            "PROTOBUF",
            "STRING"
          ],
          "dependents": [
            "schema.registry.url"
          ]
        },
        {
          "name": "key.converter.schemas.enable",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "false",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "key.converter.schemas.enable",
          "documentation": "Include schemas within each of the serialized keys. Input message keys must contain `schema` and `payload` fields and may not contain additional fields. For plain JSON data, set this to `false`. Applicable for JSON Key Converter."
        },
        {
          "name": "key.converter.replace.null.with.default",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "key.converter.replace.null.with.default",
          "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Key Converter."
        }
      ],
      "connector_configs": [
        {
          "name": "key.converter",
          "switch": {
            "input.key.format": {
              "AVRO": "io.confluent.connect.avro.AvroConverter",
              "JSON_SR": "io.confluent.connect.json.JsonSchemaConverter",
              "PROTOBUF": "io.confluent.connect.protobuf.ProtobufConverter",
              "STRING": "org.apache.kafka.connect.storage.StringConverter",
              "JSON": "org.apache.kafka.connect.json.JsonConverter",
              "BYTES": "org.apache.kafka.connect.converters.ByteArrayConverter"
            }
          }
        },
        {
          "name": "key.converter.schemas.enable"
        },
        {
          "name": "key.converter.schema.registry.url",
          "switch": {
            "input.key.format": {
              "AVRO": "${schema.registry.url}",
              "JSON_SR": "${schema.registry.url}",
              "PROTOBUF": "${schema.registry.url}"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.credentials.source",
          "switch": {
            "input.key.format": {
              "AVRO": "USER_INFO",
              "JSON_SR": "USER_INFO",
              "PROTOBUF": "USER_INFO"
            }
          }
        },
        {
          "name": "key.converter.basic.auth.user.info",
          "switch": {
            "input.key.format": {
              "AVRO": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "JSON_SR": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}",
              "PROTOBUF": "${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:username}:${file:/mnt/secrets/connect-sr-{{.logicalClusterId}}.properties:password}"
            }
          }
        },
        {
          "name": "key.converter.replace.null.with.default"
        }
      ]
    },
    {
      "template_id": "super",
      "abstract": true,
      "config_defs": [
        {
          "name": "auto.restart.on.user.error",
          "type": "BOOLEAN",
          "required": false,
          "default_value": "true",
          "importance": "MEDIUM",
          "group": "Auto-restart policy",
          "order_in_group": 1,
          "display_name": "Enable Connector Auto-restart",
          "documentation": "Enable connector to automatically restart on user-actionable errors."
        },
        {
          "name": "value.converter.enhanced.avro.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information and Enums. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data",
          "type": "BOOLEAN",
          "documentation": "Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support",
          "type": "BOOLEAN",
          "documentation": "Enable enhanced schema support to preserve package information. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to generate an index suffix for unions. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums",
          "type": "BOOLEAN",
          "documentation": "Whether to represent enums as integers. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should be specified with an optional label. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls",
          "type": "BOOLEAN",
          "documentation": "Whether to generate a struct variable for null values. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables",
          "type": "BOOLEAN",
          "documentation": "Whether nullable fields should use primitive wrapper messages. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives",
          "type": "BOOLEAN",
          "documentation": "Whether a wrapper message should be interpreted as a raw primitive at root level. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties",
          "type": "BOOLEAN",
          "documentation": "Whether to allow additional properties for object schemas. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired",
          "type": "BOOLEAN",
          "documentation": "Whether to set non-required properties to be optional. Applicable for JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format",
          "type": "STRING",
          "recommended_values": [
            "BASE64",
            "NUMERIC"
          ],
          "documentation": "Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:\nBASE64 to serialize DECIMAL logical types as base64 encoded binary data and\nNUMERIC to serialize Connect DECIMAL logical type values in JSON/JSON_SR as a number representing the decimal value.",
          "group": "Additional Configs",
          "alias": "json.output.decimal.format",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.decimal.format",
          "default_value": "BASE64"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "type": "BOOLEAN",
          "documentation": "Specify if the Serializer should attempt to register the Schema.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.auto.register.schemas"
        },
        {
          "name": "value.converter.use.latest.version",
          "type": "BOOLEAN",
          "documentation": "Use latest version of schema in subject for serialization when auto.register.schemas is false.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.use.latest.version"
        },
        {
          "name": "value.converter.latest.compatibility.strict",
          "type": "BOOLEAN",
          "documentation": "Verify latest subject version is backward compatible when `use.latest.version` is `true`.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "type": "STRING",
          "default_value": "TopicNameStrategy",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "alias": "key.subject.name.strategy",
          "documentation": "How to construct the subject name for key schema registration.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "key.converter.key.subject.name.strategy"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "TopicNameStrategy",
            "RecordNameStrategy",
            "TopicRecordNameStrategy"
          ],
          "default_value": "TopicNameStrategy",
          "alias": "subject.name.strategy,value.subject.name.strategy",
          "documentation": "Determines how to construct the subject name under which the value schema is registered with Schema Registry.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.value.subject.name.strategy"
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "type": "STRING",
          "recommended_values": [
            "DefaultReferenceSubjectNameStrategy",
            "QualifiedReferenceSubjectNameStrategy"
          ],
          "default_value": "DefaultReferenceSubjectNameStrategy",
          "documentation": "Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.reference.subject.name.strategy"
        },
        {
          "name": "value.converter.allow.optional.map.keys",
          "type": "BOOLEAN",
          "documentation": "Allow optional string map key when converting from Connect Schema to Avro Schema. Applicable for Avro Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions",
          "type": "BOOLEAN",
          "default_value": "false",
          "documentation": "Whether to flatten singleton unions. Applicable for Avro and JSON_SR Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2",
          "type": "BOOLEAN",
          "documentation": "Whether proto2 optionals are supported. Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions",
          "type": "BOOLEAN",
          "documentation": "Whether to flatten unions (oneofs). Applicable for Protobuf Converters.",
          "group": "Additional Configs",
          "required": false,
          "importance": "LOW",
          "display_name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "header.converter",
          "documentation": "The converter class for the headers. This is used to serialize and deserialize the headers of the messages.",
          "recommended_values": [
            "org.apache.kafka.connect.converters.BooleanConverter",
            "org.apache.kafka.connect.converters.ByteArrayConverter",
            "org.apache.kafka.connect.converters.DoubleConverter",
            "org.apache.kafka.connect.converters.FloatConverter",
            "org.apache.kafka.connect.converters.IntegerConverter",
            "org.apache.kafka.connect.converters.LongConverter",
            "org.apache.kafka.connect.converters.ShortConverter",
            "org.apache.kafka.connect.json.JsonConverter",
            "org.apache.kafka.connect.storage.SimpleHeaderConverter",
            "org.apache.kafka.connect.storage.StringConverter"
          ]
        }
      ],
      "connector_configs": [
        {
          "name": "auto.restart.on.user.error"
        },
        {
          "name": "value.converter.enhanced.avro.schema.support"
        },
        {
          "name": "value.converter.connect.meta.data"
        },
        {
          "name": "value.converter.enhanced.protobuf.schema.support"
        },
        {
          "name": "value.converter.generate.index.for.unions"
        },
        {
          "name": "value.converter.int.for.enums"
        },
        {
          "name": "value.converter.optional.for.nullables"
        },
        {
          "name": "value.converter.generate.struct.for.nulls"
        },
        {
          "name": "value.converter.wrapper.for.nullables"
        },
        {
          "name": "value.converter.wrapper.for.raw.primitives"
        },
        {
          "name": "value.converter.object.additional.properties"
        },
        {
          "name": "value.converter.use.optional.for.nonrequired"
        },
        {
          "name": "value.converter.decimal.format"
        },
        {
          "name": "value.converter.auto.register.schemas",
          "dynamic.mapper": {
            "name": "value.converter.auto.register.schemas.mapper"
          }
        },
        {
          "name": "value.converter.use.latest.version",
          "dynamic.mapper": {
            "name": "value.converter.use.latest.version.mapper"
          }
        },
        {
          "name": "value.converter.latest.compatibility.strict"
        },
        {
          "name": "value.converter.value.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "key.converter.key.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.value.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.reference.subject.name.strategy",
          "dynamic.mapper": {
            "name": "value.converter.reference.subject.name.strategy.mapper"
          }
        },
        {
          "name": "value.converter.allow.optional.map.keys"
        },
        {
          "name": "value.converter.flatten.singleton.unions"
        },
        {
          "name": "value.converter.optional.for.proto2"
        },
        {
          "name": "value.converter.flatten.unions"
        },
        {
          "name": "header.converter"
        },
        {
          "name": "key.converter.use.apache.http.client"
        },
        {
          "name": "value.converter.use.apache.http.client"
        }
      ]
    },
    {
      "template_id": "super-sink",
      "abstract": true,
      "config_defs": [
        {
          "name": "consumer.override.auto.offset.reset",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "consumer.override.auto.offset.reset",
          "documentation": "Defines the behavior of the consumer when there is no committed position (which occurs when the group is first initialized) or when an offset is out of range. You can choose either to reset the position to the “earliest” offset (the default) or the “latest” offset. You can also select “none” if you would rather set the initial offset yourself and you are willing to handle out of range errors manually. More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#auto-offset-reset",
          "recommended_values": [
            "earliest",
            "latest",
            "none"
          ]
        },
        {
          "name": "consumer.override.isolation.level",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Additional Configs",
          "display_name": "consumer.override.isolation.level",
          "documentation": "Controls how to read messages written transactionally. If set to read_committed, consumer.poll() will only return transactional messages which have been committed. If set to read_uncommitted (the default), consumer.poll() will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode.  More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#isolation-level",
          "recommended_values": [
            "read_committed",
            "read_uncommitted"
          ]
        },
        {
          "name": "topics.regex",
          "type": "STRING",
          "required": false,
          "importance": "LOW",
          "group": "Which topics do you want to get data from?",
          "display_name": "Topics Regex",
          "documentation": "A regular expression that matches the names of the topics to consume from. This is useful when you want to consume from multiple topics that match a certain pattern without having to list them all individually."
        }
      ],
      "connector_configs": [
        {
          "name": "consumer.override.auto.offset.reset"
        },
        {
          "name": "consumer.override.isolation.level"
        },
        {
          "name": "topics.regex"
        }
      ]
    }
  ]
}