{
  "template_id": "Hdfs2SourceConnector",
  "connector_type": "SOURCE",
  "connector.class": "io.confluent.connect.hdfs2.Hdfs2SourceConnector",
  "name": "{{.logicalClusterId}}",
  "imports": [],
  "group_order": [
    "HDFS 2 Parameters",
    "Security Parameters",
    "Connector Parameters",
    "Auto topic creation",
    "Storage Parameters",
    "Partitioner Parameters",
    "Confluent Platform license",
    "Confluent license properties",
    "License topic configuration",
    "License topic ACLs",
    "Override Default Configuration Properties"
  ],
  "config_defs": [
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 1,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 1
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 2,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 2
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 3,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 3
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 4,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 4
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 5,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 5
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 6,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 6
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 7,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 7
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 8,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 8
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 9,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 9
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 10,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 10
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 11,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 11
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 12,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 12
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 13,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 13
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 14,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 14
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 15,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 15
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 16,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 16
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 17,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 17
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 18,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 18
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 19,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 19
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 20,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 20
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 21,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 21
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 22,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 22
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 23,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 23
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 24,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 24
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 25,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 25
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 26,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 26
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 27,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 27
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 28,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 28
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 29,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 29
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 30,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 30
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 31,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 31
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 32,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 32
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 33,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 33
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 34,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 34
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 35,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 35
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 36,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 36
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 37,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 37
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 38,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 38
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 39,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 39
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 40,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 40
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 41,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 41
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 42,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 42
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 43,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 43
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 44,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 44
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 45,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 45
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 46,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 46
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 47,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 47
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 48,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 48
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 49,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 49
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 50,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 50
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 51,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 51
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 52,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 52
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 53,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 53
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 54,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 54
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 55,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 55
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 56,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 56
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 57,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 57
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 58,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 58
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 59,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 59
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 60,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 60
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 61,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 61
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 62,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 62
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 63,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 63
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 64,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 64
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 65,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 65
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 66,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 66
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 67,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 67
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 68,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 68
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 69,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 69
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 70,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 70
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 71,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 71
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 72,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 72
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 73,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 73
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 74,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 74
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 75,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 75
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 76,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 76
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 77,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 77
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 78,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 78
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 79,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 79
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 80,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 80
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 81,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 81
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 82,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 82
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 83,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 83
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 84,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 84
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 85,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 85
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 86,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 86
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 87,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 87
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 88,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 88
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 89,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 89
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 90,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 90
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 91,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 91
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 92,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 92
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 93,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 93
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 94,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 94
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 95,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 95
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 96,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 96
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 97,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 97
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 98,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 98
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 99,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 99
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 100,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 100
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 101,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 101
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 102,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 102
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 103,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 103
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 104,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 104
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 105,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 105
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 106,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 106
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 107,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 107
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 108,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 108
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 109,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 109
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 110,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 110
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 111,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 111
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 112,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 112
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 113,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 113
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 114,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 114
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 115,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 115
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 116,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 116
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 117,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 117
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 118,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 118
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 119,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 119
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 120,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 120
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 121,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 121
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 122,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 122
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 123,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 123
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 124,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 124
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 125,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 125
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 126,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 126
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 127,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 127
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 128,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 128
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 129,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 129
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 130,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 130
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 131,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 131
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 132,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 132
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 133,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 133
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 134,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 134
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 135,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 135
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 136,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 136
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 137,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 137
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 138,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 138
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 139,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 139
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 140,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 140
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 141,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 141
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 142,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 142
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 143,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 143
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 144,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 144
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 145,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 145
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 146,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 146
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 147,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 147
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 148,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 148
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 149,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 149
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 150,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 150
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 151,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 151
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 152,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 152
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 153,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 153
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 154,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 154
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 155,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 155
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 156,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 156
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 157,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 157
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 158,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 158
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 159,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 159
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 160,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 160
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 161,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 161
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 162,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 162
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 163,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 163
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 164,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 164
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 165,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 165
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 166,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 166
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 167,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 167
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 168,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 168
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 169,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 169
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 170,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 170
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 171,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 171
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 172,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 172
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 173,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 173
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 174,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 174
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 175,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 175
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 176,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 176
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 177,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 177
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 178,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 178
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 179,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 179
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 180,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 180
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 181,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 181
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 182,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 182
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 183,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 183
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 184,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 184
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 185,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 185
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 186,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 186
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 187,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 187
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 188,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 188
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 189,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 189
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 190,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 190
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 191,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 191
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 192,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 192
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 193,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 193
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 194,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 194
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 195,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 195
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 196,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 196
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 197,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 197
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 198,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 198
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 199,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 199
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 200,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 200
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 201,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 201
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 202,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 202
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 203,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 203
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 204,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 204
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 205,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 205
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 206,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 206
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 207,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 207
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 208,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 208
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 209,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 209
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 210,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 210
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 211,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 211
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 212,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 212
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 213,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 213
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 214,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 214
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 215,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 215
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 216,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 216
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 217,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 217
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 218,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 218
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 219,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 219
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 220,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 220
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 221,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 221
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 222,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 222
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 223,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 223
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 224,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 224
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 225,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 225
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 226,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 226
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 227,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 227
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 228,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 228
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 229,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 229
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 230,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 230
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 231,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 231
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 232,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 232
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 233,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 233
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 234,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 234
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 235,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 235
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 236,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 236
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 237,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 237
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 238,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 238
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 239,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 239
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 240,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 240
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 241,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 241
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 242,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 242
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 243,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 243
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 244,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 244
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 245,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 245
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 246,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 246
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 247,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 247
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 248,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 248
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 249,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 249
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 250,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 250
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 251,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 251
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 252,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 252
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 253,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 253
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 254,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 254
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 255,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 255
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 256,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 256
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 257,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 257
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 258,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 258
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 259,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 259
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 260,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 260
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 261,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 261
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 262,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 262
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 263,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 263
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 264,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 264
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 265,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 265
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 266,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 266
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 267,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 267
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 268,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 268
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 269,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 269
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 270,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 270
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 271,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 271
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 272,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 272
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 273,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 273
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 274,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 274
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 275,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 275
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 276,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 276
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 277,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 277
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 278,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 278
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 279,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 279
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 280,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 280
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 281,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 281
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 282,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 282
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 283,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 283
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 284,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 284
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 285,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 285
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 286,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 286
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 287,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 287
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 288,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 288
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 289,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 289
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 290,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 290
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 291,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 291
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 292,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 292
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 293,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 293
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 294,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 294
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 295,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 295
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 296,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 296
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 297,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 297
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 298,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 298
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 299,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 299
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 300,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 300
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 301,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 301
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 302,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 302
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 303,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 303
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 304,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 304
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 305,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 305
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 306,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 306
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 307,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 307
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 308,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 308
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 309,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 309
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 310,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 310
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 311,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 311
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 312,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 312
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 313,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 313
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 314,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 314
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 315,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 315
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 316,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 316
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 317,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 317
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 318,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 318
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 319,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 319
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 320,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 320
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 321,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 321
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 322,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 322
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 323,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 323
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 324,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 324
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 325,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 325
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 326,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 326
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 327,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 327
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 328,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 328
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 329,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 329
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 330,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 330
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 331,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 331
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 332,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 332
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 333,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 333
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 334,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 334
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "HDFS 2 Parameters",
      "order_in_group": 335,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 335
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 336,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 336
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 337,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 337
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 338,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 338
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 339,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 339
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 340,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 340
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 341,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 341
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 342,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 342
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "store.url",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 343,
      "display_name": "store.url",
      "documentation": "The HDFS connection URL. This configuration has the format ofhdfs://hostname:portand is used to make the connection with HDFS.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 343
      },
      "default": "nullimportance: high"
    },
    {
      "name": "hadoop.conf.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "HDFS 2 Parameters",
      "order_in_group": 344,
      "display_name": "hadoop.conf.dir",
      "documentation": "The Hadoop configuration directory.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 344
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.poll.interval.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "HDFS 2 Parameters",
      "order_in_group": 345,
      "display_name": "hdfs.poll.interval.ms",
      "documentation": "Frequency in milliseconds to poll for new or removed folders. This may result in updated task configurations starting to poll for data in added folders or stopping polling for data in removed folders.Type: longDefault: 300000Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 345
      },
      "default": "300000importance: medium"
    },
    {
      "name": "hdfs.authentication.kerberos",
      "type": "BOOLEANDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Security Parameters",
      "order_in_group": 346,
      "display_name": "hdfs.authentication.kerberos",
      "documentation": "Configuration indicating whether HDFS is using Kerberos for authentication.Type: booleanDefault: falseImportance: highDependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 346
      },
      "default": "falseimportance: highdependents:connect.hdfs.principal,connect.hdfs.keytab,hdfs.namenode.principal,kerberos.ticket.renew.period.ms,hadoop.conf.dir",
      "dependents": [
        "connect.hdfs.principal",
        "connect.hdfs.keytab",
        "hdfs.namenode.principal",
        "kerberos.ticket.renew.period.ms",
        "hadoop.conf.dir"
      ]
    },
    {
      "name": "connect.hdfs.keytab",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Security Parameters",
      "order_in_group": 347,
      "display_name": "connect.hdfs.keytab",
      "documentation": "The path to the keytab file for the HDFS connector principal. This keytab file should only be readable by the connector user.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 347
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "connect.hdfs.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Security Parameters",
      "order_in_group": 348,
      "display_name": "connect.hdfs.principal",
      "documentation": "The principal to use when HDFS is using Kerberos to for authentication.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 348
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "hdfs.namenode.principal",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Security Parameters",
      "order_in_group": 349,
      "display_name": "hdfs.namenode.principal",
      "documentation": "The principal for HDFS Namenode.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 349
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: high"
    },
    {
      "name": "kerberos.ticket.renew.period.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "Security Parameters",
      "order_in_group": 350,
      "display_name": "kerberos.ticket.renew.period.ms",
      "documentation": "The period in milliseconds to renew the Kerberos ticket.Type: longDefault: 3600000Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 350
      },
      "default": "3600000importance: low"
    },
    {
      "name": "format.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Connector Parameters",
      "order_in_group": 351,
      "display_name": "format.class",
      "documentation": "Class responsible for converting source objects to source records.Type: classDefault:io.confluent.connect.hdfs2.format.avro.AvroFormatImportance: highYou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.AvroFormatio.confluent.connect.hdfs2.format.json.JsonFormatio.confluent.connect.hdfs2.format.string.StringFormatio.confluent.connect.hdfs2.format.parquet.ParquetFormat",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 351
      },
      "default": "io.confluent.connect.hdfs2.format.avro.avroformatimportance: highyou can use following four type of formatter:io.confluent.connect.hdfs2.format.avro.avroformatio.confluent.connect.hdfs2.format.json.jsonformatio.confluent.connect.hdfs2.format.string.stringformatio.confluent.connect.hdfs2.format.parquet.parquetformat"
    },
    {
      "name": "schema.cache.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "Connector Parameters",
      "order_in_group": 352,
      "display_name": "schema.cache.size",
      "documentation": "The size of the schema cache used in the Avro converter.Type: intDefault: 50Valid Values: [1,\u00e2\u0080\u00a6]Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 352
      },
      "default": "50valid values: [1,\u00e2\u0080\u00a6]importance: low",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "record.batch.max.size",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Connector Parameters",
      "order_in_group": 353,
      "display_name": "record.batch.max.size",
      "documentation": "The maximum amount of records to return each time HDFS2 is polled.Type: intDefault: 200Valid Values: [1,\u00e2\u0080\u00a6]Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 353
      },
      "default": "200valid values: [1,\u00e2\u0080\u00a6]importance: medium",
      "valid_values": [
        "1",
        "\u00e2\u0080\u00a6"
      ]
    },
    {
      "name": "topic.creation.groups",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 354,
      "display_name": "topic.creation.groups",
      "documentation": "A list of group aliases that are used to define per-group topic configurations for matching topics. Adefaultgroup always exists and matches all topics.Type: List of String typesDefault: emptyPossible Values: The values of this property refer to any additional groups. Adefaultgroup is always defined for topic configurations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 354
      },
      "default": "emptypossible values: the values of this property refer to any additional groups. adefaultgroup is always defined for topic configurations."
    },
    {
      "name": "topic.creation.$alias.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 355,
      "display_name": "topic.creation.$alias.replication.factor",
      "documentation": "The replication factor for new topics created by the connector. This value must not be larger than the number of brokers in the Kafka cluster. If this value is larger than the number of Kafka brokers, an error occurs when the connector attempts to create a topic. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 355
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.partitions",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 356,
      "display_name": "topic.creation.$alias.partitions",
      "documentation": "The number of topic partitions created by this connector. This is arequired propertyfor thedefaultgroup. This property is optional for any other group defined intopic.creation.groups. Other groups use the Kafka broker default value.Type: intDefault: n/aPossible Values:>=1for a specific valid value or-1to use the Kafka broker\u00e2\u0080\u0099s default value.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 356
      },
      "default": "n/apossible values:>=1for a specific valid value or-1to use the kafka broker\u00e2\u0080\u0099s default value."
    },
    {
      "name": "topic.creation.$alias.include",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 357,
      "display_name": "topic.creation.$alias.include",
      "documentation": "A list of strings that represent regular expressions that match topic names. This list is used to include topics with matching values, and apply this group\u00e2\u0080\u0099s specific configuration to the matching topics.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 357
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.exclude",
      "type": "LIST",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 358,
      "display_name": "topic.creation.$alias.exclude",
      "documentation": "A list of strings representing regular expressions that match topic names. This list is used to exclude topics with matching values from getting the group\u00e2\u0080\u0099s specfic configuration.$aliasapplies to any group defined intopic.creation.groups. This property does not apply to thedefaultgroup. Note that exclusion rules override any inclusion rules for topics.Type: List of String typesDefault: emptyPossible Values: Comma-separated list of exact topic names or regular expressions.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 358
      },
      "default": "emptypossible values: comma-separated list of exact topic names or regular expressions."
    },
    {
      "name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "type": "PROPERTY",
      "required": false,
      "importance": "MEDIUM",
      "group": "Auto topic creation",
      "order_in_group": 359,
      "display_name": "topic.creation.$alias.${kafkaTopicSpecificConfigName}",
      "documentation": "Any of theChanging Broker Configurations Dynamicallyfor the version of the Kafka broker where the records will be written. The broker\u00e2\u0080\u0099s topic-level configuration value is used if the configuration is not specified for the rule.$aliasapplies to thedefaultgroup as well as any group defined intopic.creation.groups.Type: property valuesDefault: Kafka broker value",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 359
      },
      "default": "kafka broker value"
    },
    {
      "name": "topics.dir",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Storage Parameters",
      "order_in_group": 360,
      "display_name": "topics.dir",
      "documentation": "Top level directory where data was stored to be re-ingested by Kafka.Type: stringDefault: topicsImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 360
      },
      "default": "topicsimportance: high"
    },
    {
      "name": "directory.delim",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Storage Parameters",
      "order_in_group": 361,
      "display_name": "directory.delim",
      "documentation": "Directory delimiter pattern.Type: stringDefault: /Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 361
      },
      "default": "/importance: medium"
    },
    {
      "name": "behavior.on.error",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Storage Parameters",
      "order_in_group": 362,
      "display_name": "behavior.on.error",
      "documentation": "Sets how the connector handles errors that occur when processing records.Type: stringDefault: failValid Values:fail,ignore,logImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 362
      },
      "default": "failvalid values:fail,ignore,logimportance: medium"
    },
    {
      "name": "partitioner.class",
      "type": "CLASSDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Partitioner Parameters",
      "order_in_group": 363,
      "display_name": "partitioner.class",
      "documentation": "The partitioner to use when reading data to the store. The following\npartitioners are available:io.confluent.connect.storage.partitioner.DefaultPartitionerio.confluent.connect.storage.partitioner.DailyPartitionerio.confluent.connect.storage.partitioner.HourlyPartitionerio.confluent.connect.storage.partitioner.FieldPartitionerio.confluent.connect.storage.partitioner.TimeBasedPartitionerType: classDefault: io.confluent.connect.storage.partitioner.DefaultPartitionerImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 363
      },
      "default": "io.confluent.connect.storage.partitioner.defaultpartitionerimportance: high"
    },
    {
      "name": "partition.field.name",
      "type": "LISTDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 364,
      "display_name": "partition.field.name",
      "documentation": "The name of the partitioning field when FieldPartitioner is used.Type: listDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 364
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "path.format",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 365,
      "display_name": "path.format",
      "documentation": "This configuration that was used to set the format of the data directories\nwhen partitioning with a TimeBasedPartitioner. For example, if you setpath.formatto'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, then a valid\ndata directories would be:/year=2015/month=12/day=07/hour=15/.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 365
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "partition.duration.ms",
      "type": "LONGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 366,
      "display_name": "partition.duration.ms",
      "documentation": "The duration of a partition milliseconds used byTimeBasedPartitioner. The\ndefault value -1 means that we are not usingTimeBasedPartitioner.Type: longDefault: -1Importance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 366
      },
      "default": "-1importance: medium"
    },
    {
      "name": "locale",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 367,
      "display_name": "locale",
      "documentation": "The locale to use when partitioning withTimeBasedPartitioner, and used to\nformat dates and times. For example, useen-USfor US English,en-GBfor UK English, orfr-FRfor French (in France). These may vary by Java\nversion; see theavailable locales.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 367
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timezone",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 368,
      "display_name": "timezone",
      "documentation": "The timezone to use when partitioning withTimeBasedPartitioner. Used to\nformat and compute dates and times. All timezone IDs must be specified in the\nlong format, such asAmerica/Los_Angeles,America/New_York, andEurope/Paris, orUTC. Alternatively a locale independent, fixed\noffset, datetime zone can be specified in form[+-]hh:mm. Support for\nthese timezones may vary by Java version. See theavailable timezones within\neach locale, such asthose within the\nUS English locale.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 368
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "timestamp.extractor",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 369,
      "display_name": "timestamp.extractor",
      "documentation": "The extractor that gets the timestamp for records when partitioning withTimeBasedPartitioner. It can be set toWallclock,RecordorRecordFieldin order to use one of the built-in timestamp extractors or be\ngiven the fully-qualified class name of a user-defined class that extends theTimestampExtractorinterface.Type: stringDefault: WallclockImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 369
      },
      "default": "wallclockimportance: medium"
    },
    {
      "name": "timestamp.field",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Partitioner Parameters",
      "order_in_group": 370,
      "display_name": "timestamp.field",
      "documentation": "The record field to be used as timestamp by the timestamp extractor.Type: stringDefault: timestampImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 370
      },
      "default": "timestampimportance: medium"
    },
    {
      "name": "confluent.topic.bootstrap.servers",
      "type": "LISTIMPORTANCE",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 371,
      "display_name": "confluent.topic.bootstrap.servers",
      "documentation": "A list of host/port pairs to use for establishing the initial connection to the Kafka cluster used for licensing. All servers in the cluster will be discovered from the initial connection. This list should be in the formhost1:port1,host2:port2,.... These servers are used only for the initial connection to discover the full cluster membership, which may change dynamically, so this list need not contain the full set of servers. You may want more than one, in case a server is down.Type: listImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 371
      }
    },
    {
      "name": "confluent.topic",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "Confluent Platform license",
      "order_in_group": 372,
      "display_name": "confluent.topic",
      "documentation": "Name of the Kafka topic used for Confluent Platform configuration, including licensing information.Type: stringDefault: _confluent-commandImportance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 372
      },
      "default": "_confluent-commandimportance: low"
    },
    {
      "name": "confluent.topic.replication.factor",
      "type": "INTDEFAULT",
      "required": false,
      "importance": "LOW",
      "group": "Confluent Platform license",
      "order_in_group": 373,
      "display_name": "confluent.topic.replication.factor",
      "documentation": "The replication factor for the Kafka topic used for Confluent Platform configuration, including licensing information. This is used only if the topic does not already exist, and the default of 3 is appropriate for production use. If you are using a development environment with less than 3 brokers, you must set this to the number of brokers (often 1).Type: intDefault: 3Importance: low",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 373
      },
      "default": "3importance: low"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 374,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 374
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 375,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 375
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 376,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 376
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 377,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 377
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 378,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 378
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent Platform license",
      "order_in_group": 379,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 379
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Confluent Platform license",
      "order_in_group": 380,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 380
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "confluent.license",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 381,
      "display_name": "confluent.license",
      "documentation": "Confluent issues enterprise license keys to each subscriber. The license key is text that you can copy and\npaste as the value forconfluent.license. A trial license allows using the connector for a 30-day trial period. A developer license allows using the connector indefinitely for single-broker development environments.If you are a subscriber, contact Confluent Support for more information.Type: stringDefault: \u00e2\u0080\u009c\u00e2\u0080\u009dValid Values: Confluent Platform licenseImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 381
      },
      "default": "\u00e2\u0080\u009c\u00e2\u0080\u009dvalid values: confluent platform licenseimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 382,
      "display_name": "confluent.topic.ssl.truststore.location",
      "documentation": "The location of the trust store file.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 382
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.truststore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 383,
      "display_name": "confluent.topic.ssl.truststore.password",
      "documentation": "The password for the trust store file. If a password is not set access to the truststore is still available, but\nintegrity checking is disabled.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 383
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.location",
      "type": "STRINGDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 384,
      "display_name": "confluent.topic.ssl.keystore.location",
      "documentation": "The location of the key store file. This is optional for client and can be used for two-way authentication for client.Type: stringDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 384
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.keystore.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 385,
      "display_name": "confluent.topic.ssl.keystore.password",
      "documentation": "The store password for the key store file. This is optional for client and only needed if ssl.keystore.location is configured.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 385
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.ssl.key.password",
      "type": "PASSWORDDEFAULT",
      "required": true,
      "importance": "HIGH",
      "group": "Confluent license properties",
      "order_in_group": 386,
      "display_name": "confluent.topic.ssl.key.password",
      "documentation": "The password of the private key in the key store file. This is optional for client.Type: passwordDefault: nullImportance: high",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 386
      },
      "default": "nullimportance: high"
    },
    {
      "name": "confluent.topic.security.protocol",
      "type": "STRINGDEFAULT",
      "required": false,
      "importance": "MEDIUM",
      "group": "Confluent license properties",
      "order_in_group": 387,
      "display_name": "confluent.topic.security.protocol",
      "documentation": "Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.Type: stringDefault: \u00e2\u0080\u009cPLAINTEXT\u00e2\u0080\u009dImportance: medium",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 387
      },
      "default": "\u00e2\u0080\u009cplaintext\u00e2\u0080\u009dimportance: medium"
    },
    {
      "name": "Search by configuration property name",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 2,
      "display_name": "Search by configuration property name",
      "documentation": "Enter a string to search and filter by configuration property name.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 2
      }
    },
    {
      "name": "name",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 2,
      "display_name": "name",
      "documentation": "Globally unique name to use for this connector.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 2
      }
    },
    {
      "name": "connector.class",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 3,
      "display_name": "connector.class",
      "documentation": "Name or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector. If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name,  or use \u00e2\u0080\u009cFileStreamSink\u00e2\u0080\u009d or \u00e2\u0080\u009cFileStreamSinkConnector\u00e2\u0080\u009d to make the configuration a bit shorter",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 3
      }
    },
    {
      "name": "tasks.max",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 4,
      "display_name": "tasks.max",
      "documentation": "Maximum number of tasks to use for this connector.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 4
      }
    },
    {
      "name": "tasks.max.enforce",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 5,
      "display_name": "tasks.max.enforce",
      "documentation": "(Deprecated) Whether to enforce that the tasks.max property is respected by the connector. By default, connectors that generate too many tasks will fail, and existing sets of tasks that exceed the tasks.max property will also be failed. If this property is set to false, then connectors will be allowed to generate more than the maximum number of tasks, and existing sets of tasks that exceed the tasks.max property will be allowed to run. This property is deprecated and will be removed in an upcoming major release.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 5
      }
    },
    {
      "name": "key.converter",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 6,
      "display_name": "key.converter",
      "documentation": "Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 6
      }
    },
    {
      "name": "value.converter",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 7,
      "display_name": "value.converter",
      "documentation": "Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 7
      }
    },
    {
      "name": "header.converter",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 8,
      "display_name": "header.converter",
      "documentation": "HeaderConverter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the header values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro. By default, the SimpleHeaderConverter is used to serialize header values to strings and deserialize them by inferring the schemas.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 8
      }
    },
    {
      "name": "config.action.reload",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 9,
      "display_name": "config.action.reload",
      "documentation": "The action that Connect should take on the connector when changes in external configuration providers result in a change in the connector\u00e2\u0080\u0099s configuration properties. A value of \u00e2\u0080\u0098none\u00e2\u0080\u0099 indicates that Connect will do nothing. A value of \u00e2\u0080\u0098restart\u00e2\u0080\u0099 indicates that Connect should restart/reload the connector with the updated configuration properties.The restart may actually be scheduled in the future if the external configuration provider indicates that a configuration value will expire in the future.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 9
      }
    },
    {
      "name": "transforms",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 10,
      "display_name": "transforms",
      "documentation": "Aliases for the transformations to be applied to records.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 10
      }
    },
    {
      "name": "predicates",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 11,
      "display_name": "predicates",
      "documentation": "Aliases for the predicates used by transformations.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 11
      }
    },
    {
      "name": "errors.retry.timeout",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 12,
      "display_name": "errors.retry.timeout",
      "documentation": "The maximum duration in milliseconds that a failed operation will be reattempted. The default is 0, which means no retries will be attempted. Use -1 for infinite retries.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 12
      }
    },
    {
      "name": "errors.retry.delay.max.ms",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 13,
      "display_name": "errors.retry.delay.max.ms",
      "documentation": "The maximum duration in milliseconds between consecutive retry attempts. Jitter will be added to the delay once this limit is reached to prevent thundering herd issues.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 13
      }
    },
    {
      "name": "errors.tolerance",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 14,
      "display_name": "errors.tolerance",
      "documentation": "Behavior for tolerating errors during connector operation. \u00e2\u0080\u0098none\u00e2\u0080\u0099 is the default value and signals that any error will result in an immediate connector task failure; \u00e2\u0080\u0098all\u00e2\u0080\u0099 changes the behavior to skip over problematic records.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 14
      }
    },
    {
      "name": "errors.log.enable",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 15,
      "display_name": "errors.log.enable",
      "documentation": "If true, write each error and the details of the failed operation and problematic record to the Connect application log. This is \u00e2\u0080\u0098false\u00e2\u0080\u0099 by default, so that only errors that are not tolerated are reported.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 15
      }
    },
    {
      "name": "errors.log.include.messages",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 16,
      "display_name": "errors.log.include.messages",
      "documentation": "Whether to include in the log the Connect record that resulted in a failure. For sink records, the topic, partition, offset, and timestamp will be logged. For source records, the key and value (and their schemas), all headers, and the timestamp, Kafka topic, Kafka partition, source partition, and source offset will be logged. This is \u00e2\u0080\u0098false\u00e2\u0080\u0099 by default, which will prevent record keys, values, and headers from being written to log files.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 16
      }
    },
    {
      "name": "exactly.once.support",
      "type": "STRING",
      "required": true,
      "importance": "HIGH",
      "group": "Common",
      "order_in_group": 18,
      "display_name": "exactly.once.support",
      "documentation": "Permitted values are requested, required. If set to \u00e2\u0080\u009crequired\u00e2\u0080\u009d, forces a preflight check for the connector to ensure that it can provide exactly-once semantics with the given configuration. Some connectors may be capable of providing exactly-once semantics but not signal to Connect that they support this; in that case, documentation for the connector should be consulted carefully before creating it, and the value for this property should be set to \u00e2\u0080\u009crequested\u00e2\u0080\u009d. Additionally, if the value is set to \u00e2\u0080\u009crequired\u00e2\u0080\u009d but the worker that performs preflight validation does not have exactly-once support enabled for source connectors, requests to create or validate the connector will fail.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 18
      }
    },
    {
      "name": "transaction.boundary",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 19,
      "display_name": "transaction.boundary",
      "documentation": "Permitted values are: poll, interval, connector. If set to \u00e2\u0080\u0098poll\u00e2\u0080\u0099, a new producer transaction will be started and committed for every batch of records that each task from this connector provides to Connect. If set to \u00e2\u0080\u0098connector\u00e2\u0080\u0099, relies on connector-defined transaction boundaries; note that not all connectors are capable of defining their own transaction boundaries, and in that case, attempts to instantiate a connector with this value will fail. Finally, if set to \u00e2\u0080\u0098interval\u00e2\u0080\u0099, commits transactions only after a user-defined time interval has passed.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 19
      }
    },
    {
      "name": "transaction.boundary.interval.ms",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 20,
      "display_name": "transaction.boundary.interval.ms",
      "documentation": "If \u00e2\u0080\u0098transaction.boundary\u00e2\u0080\u0099 is set to \u00e2\u0080\u0098interval\u00e2\u0080\u0099, determines the interval for producer transaction commits by connector tasks. If unset, defaults to the value of the worker-level \u00e2\u0080\u0098offset.flush.interval.ms\u00e2\u0080\u0099 property. It has no effect if a different transaction.boundary is specified.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 20
      }
    },
    {
      "name": "offsets.storage.topic",
      "type": "STRING",
      "required": false,
      "importance": "MEDIUM",
      "group": "Common",
      "order_in_group": 21,
      "display_name": "offsets.storage.topic",
      "documentation": "The name of a separate offsets topic to use for this connector. If empty or not specified, the worker\u00e2\u0080\u0099s global offsets topic name will be used. If specified, the offsets topic will be created if it does not already exist on the Kafka cluster targeted by this connector (which may be different from the one used for the worker\u00e2\u0080\u0099s global offsets topic if the bootstrap.servers property of the connector\u00e2\u0080\u0099s producer has been overridden from the worker\u00e2\u0080\u0099s). Only applicable in distributed mode; in standalone mode, setting this property will have no effect.",
      "validators": [],
      "sanitizers": [],
      "metadata": {
        "page": "CONFIGURATION",
        "order_in_page": 21
      }
    }
  ]
}